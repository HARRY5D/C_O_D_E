{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f4d515",
   "metadata": {},
   "source": [
    "# TensorFlow Lite Model Evaluator\n",
    "\n",
    "This notebook provides comprehensive evaluation and testing capabilities for TensorFlow Lite (`.tflite`) models. It can:\n",
    "\n",
    "## üéØ **Key Features:**\n",
    "1. **Load and inspect** any `.tflite` model\n",
    "2. **Analyze model architecture** (input/output shapes, data types)\n",
    "3. **Generate comprehensive test cases** for various scenarios\n",
    "4. **Evaluate model accuracy** on different datasets\n",
    "5. **Performance benchmarking** (inference speed, memory usage)\n",
    "6. **Visual analysis** of predictions and confidence scores\n",
    "\n",
    "## üìä **Supported Model Types:**\n",
    "- Text classification models (SMS, email, document classification)\n",
    "- Image classification models\n",
    "- Regression models\n",
    "- Custom trained models\n",
    "\n",
    "## üîß **Usage:**\n",
    "Simply provide the path to your `.tflite` model file and any relevant tokenizer/preprocessor files, and this notebook will automatically analyze and test your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa380fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and TensorFlow Lite\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üíª GPU Available: {len(tf.config.list_physical_devices('GPU'))} device(s)\")\n",
    "print(\"üîá Warnings suppressed for cleaner output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33fe32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths to your model files\n",
    "MODEL_CONFIG = {\n",
    "    # üéØ MAIN MODEL FILE (Required)\n",
    "    'tflite_model_path': r\"D:\\JAVA\\CODE\\PYTHON\\ML\\Secure_Chat_Lite\\sms_phishing_model.tflite\",\n",
    "    \n",
    "    # üìù TEXT PREPROCESSING FILES (Optional - for text models)\n",
    "    'tokenizer_path': r\"D:\\JAVA\\CODE\\PYTHON\\ML\\Secure_Chat_Lite\\tokenizer.pickle\",\n",
    "    \n",
    "    # ‚öôÔ∏è MODEL PARAMETERS (Auto-detected or manually set)\n",
    "    'max_sequence_length': 100,\n",
    "    'model_type': 'auto',  # 'text_classification', 'image_classification', 'regression', 'auto'\n",
    "    \n",
    "    # üß™ TEST CONFIGURATION\n",
    "    'num_test_samples': 1000,\n",
    "    'benchmark_iterations': 100,\n",
    "    'confidence_threshold': 0.5,\n",
    "}\n",
    "\n",
    "# üìÅ ALTERNATIVE PATHS - Uncomment and modify if your files are elsewhere\n",
    "# MODEL_CONFIG['tflite_model_path'] = r\"path\\to\\your\\model.tflite\"\n",
    "# MODEL_CONFIG['tokenizer_path'] = r\"path\\to\\your\\tokenizer.pickle\"\n",
    "\n",
    "print(\"üìã Configuration loaded successfully!\")\n",
    "print(f\"üéØ Model Path: {MODEL_CONFIG['tflite_model_path']}\")\n",
    "print(f\"üìù Tokenizer Path: {MODEL_CONFIG['tokenizer_path']}\")\n",
    "print(f\"‚öôÔ∏è Model Type: {MODEL_CONFIG['model_type']}\")\n",
    "print(f\"üß™ Test Samples: {MODEL_CONFIG['num_test_samples']}\")\n",
    "\n",
    "# Verify file existence\n",
    "model_exists = os.path.exists(MODEL_CONFIG['tflite_model_path'])\n",
    "tokenizer_exists = os.path.exists(MODEL_CONFIG['tokenizer_path']) if MODEL_CONFIG['tokenizer_path'] else False\n",
    "\n",
    "print(f\"\\nüìä File Status:\")\n",
    "print(f\"{'‚úÖ' if model_exists else '‚ùå'} TFLite Model: {model_exists}\")\n",
    "print(f\"{'‚úÖ' if tokenizer_exists else '‚ùå'} Tokenizer: {tokenizer_exists}\")\n",
    "\n",
    "if not model_exists:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: TFLite model file not found!\")\n",
    "    print(\"   Please update the 'tflite_model_path' in the configuration above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6917cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Lite Model Analysis and Inspection\n",
    "class TFLiteModelAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize the TensorFlow Lite model analyzer\"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "        self.model_info = {}\n",
    "        self.load_model()\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load and initialize the TensorFlow Lite model\"\"\"\n",
    "        try:\n",
    "            self.interpreter = tf.lite.Interpreter(model_path=self.model_path)\n",
    "            self.interpreter.allocate_tensors()\n",
    "            \n",
    "            # Get input and output details\n",
    "            self.input_details = self.interpreter.get_input_details()\n",
    "            self.output_details = self.interpreter.get_output_details()\n",
    "            \n",
    "            print(f\"‚úÖ Model loaded successfully from: {self.model_path}\")\n",
    "            self.analyze_model_structure()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def analyze_model_structure(self):\n",
    "        \"\"\"Analyze and extract model structure information\"\"\"\n",
    "        # Get model size\n",
    "        model_size = os.path.getsize(self.model_path) / (1024 * 1024)  # MB\n",
    "        \n",
    "        # Analyze inputs\n",
    "        input_info = []\n",
    "        for i, input_detail in enumerate(self.input_details):\n",
    "            input_info.append({\n",
    "                'index': i,\n",
    "                'name': input_detail['name'],\n",
    "                'shape': input_detail['shape'].tolist(),\n",
    "                'dtype': str(input_detail['dtype']),\n",
    "                'quantization': input_detail['quantization']\n",
    "            })\n",
    "        \n",
    "        # Analyze outputs\n",
    "        output_info = []\n",
    "        for i, output_detail in enumerate(self.output_details):\n",
    "            output_info.append({\n",
    "                'index': i,\n",
    "                'name': output_detail['name'],\n",
    "                'shape': output_detail['shape'].tolist(),\n",
    "                'dtype': str(output_detail['dtype']),\n",
    "                'quantization': output_detail['quantization']\n",
    "            })\n",
    "        \n",
    "        self.model_info = {\n",
    "            'model_size_mb': model_size,\n",
    "            'input_count': len(self.input_details),\n",
    "            'output_count': len(self.output_details),\n",
    "            'inputs': input_info,\n",
    "            'outputs': output_info\n",
    "        }\n",
    "        \n",
    "        # Auto-detect model type\n",
    "        self.detect_model_type()\n",
    "    \n",
    "    def detect_model_type(self):\n",
    "        \"\"\"Auto-detect the type of model based on input/output shapes\"\"\"\n",
    "        input_shape = self.input_details[0]['shape']\n",
    "        output_shape = self.output_details[0]['shape']\n",
    "        \n",
    "        if len(input_shape) == 2 and input_shape[1] > 1:\n",
    "            # Likely text classification (batch_size, sequence_length)\n",
    "            self.model_info['detected_type'] = 'text_classification'\n",
    "        elif len(input_shape) == 4:\n",
    "            # Likely image classification (batch_size, height, width, channels)\n",
    "            self.model_info['detected_type'] = 'image_classification'\n",
    "        elif len(output_shape) == 2 and output_shape[1] == 1:\n",
    "            # Likely regression (batch_size, 1)\n",
    "            self.model_info['detected_type'] = 'regression'\n",
    "        elif len(output_shape) == 2 and output_shape[1] > 1:\n",
    "            # Likely classification (batch_size, num_classes)\n",
    "            self.model_info['detected_type'] = 'classification'\n",
    "        else:\n",
    "            self.model_info['detected_type'] = 'unknown'\n",
    "    \n",
    "    def print_model_summary(self):\n",
    "        \"\"\"Print a comprehensive model summary\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"üîç TENSORFLOW LITE MODEL ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"üìÅ Model File: {os.path.basename(self.model_path)}\")\n",
    "        print(f\"üíæ Model Size: {self.model_info['model_size_mb']:.2f} MB\")\n",
    "        print(f\"ü§ñ Detected Type: {self.model_info['detected_type']}\")\n",
    "        print(f\"üî¢ Input Count: {self.model_info['input_count']}\")\n",
    "        print(f\"üî¢ Output Count: {self.model_info['output_count']}\")\n",
    "        \n",
    "        print(f\"\\nüì• INPUT DETAILS:\")\n",
    "        for inp in self.model_info['inputs']:\n",
    "            print(f\"  ‚Ä¢ {inp['name']}: {inp['shape']} ({inp['dtype']})\")\n",
    "        \n",
    "        print(f\"\\nüì§ OUTPUT DETAILS:\")\n",
    "        for out in self.model_info['outputs']:\n",
    "            print(f\"  ‚Ä¢ {out['name']}: {out['shape']} ({out['dtype']})\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"Run inference on input data\"\"\"\n",
    "        # Ensure input data has correct shape and type\n",
    "        if isinstance(input_data, np.ndarray):\n",
    "            input_data = input_data.astype(self.input_details[0]['dtype'])\n",
    "            if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "                input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # Set input tensor\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        \n",
    "        # Run inference\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Get output\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        \n",
    "        return output_data\n",
    "\n",
    "# Initialize the model analyzer\n",
    "if model_exists:\n",
    "    analyzer = TFLiteModelAnalyzer(MODEL_CONFIG['tflite_model_path'])\n",
    "    analyzer.print_model_summary()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping model analysis - model file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing and Tokenizer Functions\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, tokenizer_path=None, max_length=100):\n",
    "        \"\"\"Initialize text preprocessor with optional tokenizer\"\"\"\n",
    "        self.tokenizer = None\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        \n",
    "        if tokenizer_path and os.path.exists(tokenizer_path):\n",
    "            self.load_tokenizer(tokenizer_path)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No tokenizer file found - will use basic text preprocessing\")\n",
    "    \n",
    "    def load_tokenizer(self, tokenizer_path):\n",
    "        \"\"\"Load the tokenizer from pickle file\"\"\"\n",
    "        try:\n",
    "            with open(tokenizer_path, 'rb') as f:\n",
    "                self.tokenizer = pickle.load(f)\n",
    "            print(f\"‚úÖ Tokenizer loaded from: {tokenizer_path}\")\n",
    "            print(f\"üìù Vocabulary size: {len(self.tokenizer.word_index) if hasattr(self.tokenizer, 'word_index') else 'Unknown'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove phone numbers (basic pattern)\n",
    "        text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
    "        \n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text for model input\"\"\"\n",
    "        # Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        if not cleaned_text:\n",
    "            # Return zero array for empty text\n",
    "            return np.zeros((1, self.max_length), dtype=np.float32)\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            # Use loaded tokenizer\n",
    "            sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n",
    "            padded = pad_sequences(sequence, maxlen=self.max_length, padding='post', truncating='post')\n",
    "            return padded.astype(np.float32)\n",
    "        else:\n",
    "            # Basic preprocessing - convert to simple character indices\n",
    "            # This is a fallback when no tokenizer is available\n",
    "            char_to_int = {chr(i): i-96 for i in range(97, 123)}  # a-z -> 1-26\n",
    "            char_to_int[' '] = 27\n",
    "            \n",
    "            sequence = [char_to_int.get(char, 0) for char in cleaned_text[:self.max_length]]\n",
    "            sequence += [0] * (self.max_length - len(sequence))  # Pad\n",
    "            \n",
    "            return np.array([sequence], dtype=np.float32)\n",
    "    \n",
    "    def preprocess_batch(self, texts):\n",
    "        \"\"\"Preprocess a batch of texts\"\"\"\n",
    "        return np.vstack([self.preprocess_text(text) for text in texts])\n",
    "\n",
    "# Initialize text preprocessor\n",
    "if model_exists and analyzer.model_info['detected_type'] in ['text_classification', 'classification']:\n",
    "    max_seq_length = MODEL_CONFIG['max_sequence_length']\n",
    "    if 'inputs' in analyzer.model_info and len(analyzer.model_info['inputs']) > 0:\n",
    "        input_shape = analyzer.model_info['inputs'][0]['shape']\n",
    "        if len(input_shape) >= 2:\n",
    "            max_seq_length = input_shape[1] if input_shape[1] > 0 else max_seq_length\n",
    "    \n",
    "    text_preprocessor = TextPreprocessor(\n",
    "        tokenizer_path=MODEL_CONFIG['tokenizer_path'],\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    print(f\"üìù Text preprocessor initialized with max_length: {max_seq_length}\")\n",
    "else:\n",
    "    text_preprocessor = None\n",
    "    print(\"‚ö†Ô∏è  Text preprocessor not needed for this model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79295e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Test Case Generation\n",
    "class TestCaseGenerator:\n",
    "    def __init__(self, model_type='auto'):\n",
    "        \"\"\"Initialize test case generator\"\"\"\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def generate_text_classification_tests(self):\n",
    "        \"\"\"Generate test cases for text classification models\"\"\"\n",
    "        test_cases = {\n",
    "            'phishing_suspicious': [\n",
    "                \"URGENT: Your account will be suspended! Click here to verify: http://fake-bank.com\",\n",
    "                \"Congratulations! You've won $10,000! Claim now by calling 1-800-FAKE\",\n",
    "                \"Your bank account has been compromised. Update password at fake-site.com\",\n",
    "                \"Free iPhone 14! Limited time offer. Click link to claim your prize now!\",\n",
    "                \"ALERT: Suspicious activity detected. Verify identity to prevent closure\",\n",
    "                \"You have received a tax refund of $2,500. Click to claim: irs-refund.fake\",\n",
    "                \"Your package delivery failed. Pay $5 shipping fee to reschedule\",\n",
    "                \"Account locked due to security breach. Unlock now: secure-bank.fake\",\n",
    "                \"WINNER! You've been selected for $5000 cash prize. Call immediately!\",\n",
    "                \"Credit card payment failed. Update details to avoid service suspension\"\n",
    "            ],\n",
    "            \n",
    "            'legitimate_safe': [\n",
    "                \"Hi! How are you doing today? Hope you're well and staying safe!\",\n",
    "                \"Don't forget about our meeting tomorrow at 3 PM in conference room\",\n",
    "                \"Thanks for the delicious dinner last night! Had a wonderful time\",\n",
    "                \"Can you pick up milk from the store on your way home please?\",\n",
    "                \"Happy birthday! Hope you have a wonderful day celebrating!\",\n",
    "                \"The weather is beautiful today. Perfect for a walk in the park!\",\n",
    "                \"Great job on the presentation today. Very well prepared and delivered\",\n",
    "                \"Looking forward to our vacation next week. Should be relaxing!\",\n",
    "                \"Reminder: Doctor appointment scheduled for Friday at 2:30 PM\",\n",
    "                \"Thank you for helping with the project. Really appreciate it!\"\n",
    "            ],\n",
    "            \n",
    "            'edge_cases': [\n",
    "                \"\",  # Empty message\n",
    "                \"a\",  # Single character\n",
    "                \"OK\",  # Very short message\n",
    "                \"No\",  # Another short message\n",
    "                \"Yes, sure thing!\",  # Short but complete\n",
    "                \"AAAAAAAAAA\" * 20,  # Very long repetitive message\n",
    "                \"123456789\",  # Only numbers\n",
    "                \"!@#$%^&*()\",  # Only special characters\n",
    "                \"Hello! Visit our website: www.legitimate-business.com for info\",  # Legitimate with URL\n",
    "                \"Meeting at 3pm. Call if you need directions or have questions.\"  # Normal business\n",
    "            ],\n",
    "            \n",
    "            'mixed_scenarios': [\n",
    "                \"Your order #12345 has been shipped. Track at: realstore.com/track\",\n",
    "                \"Payment confirmation: $25.99 charged to your card ending in 4567\",\n",
    "                \"Security alert: New device logged into your account from New York\",\n",
    "                \"Reminder: Your subscription expires in 3 days. Renew to continue\",\n",
    "                \"Welcome to our service! Here's your verification code: 123456\",\n",
    "                \"System maintenance scheduled for tonight 11 PM - 1 AM EST\",\n",
    "                \"Your booking confirmation: Hotel XYZ, Check-in: March 15th\",\n",
    "                \"Password reset requested. If this wasn't you, ignore this message\",\n",
    "                \"Thank you for your purchase! Receipt attached. Support: help@store.com\",\n",
    "                \"Flight delay notification: UA123 delayed by 45 minutes\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return test_cases\n",
    "    \n",
    "    def generate_image_classification_tests(self):\n",
    "        \"\"\"Generate test cases for image classification models\"\"\"\n",
    "        # For image models, we'll generate synthetic data\n",
    "        input_shape = analyzer.input_details[0]['shape']\n",
    "        if len(input_shape) == 4:  # (batch, height, width, channels)\n",
    "            height, width, channels = input_shape[1], input_shape[2], input_shape[3]\n",
    "        else:\n",
    "            height, width, channels = 224, 224, 3  # Default\n",
    "        \n",
    "        test_images = {\n",
    "            'random_noise': np.random.rand(10, height, width, channels).astype(np.float32),\n",
    "            'zeros': np.zeros((5, height, width, channels), dtype=np.float32),\n",
    "            'ones': np.ones((5, height, width, channels), dtype=np.float32),\n",
    "            'pattern': np.tile(np.arange(height*width*channels).reshape(height, width, channels), (5, 1, 1, 1)).astype(np.float32)\n",
    "        }\n",
    "        \n",
    "        return test_images\n",
    "    \n",
    "    def generate_regression_tests(self):\n",
    "        \"\"\"Generate test cases for regression models\"\"\"\n",
    "        input_shape = analyzer.input_details[0]['shape']\n",
    "        feature_count = input_shape[1] if len(input_shape) >= 2 else 10\n",
    "        \n",
    "        test_data = {\n",
    "            'random_normal': np.random.normal(0, 1, (100, feature_count)).astype(np.float32),\n",
    "            'random_uniform': np.random.uniform(-1, 1, (50, feature_count)).astype(np.float32),\n",
    "            'zeros': np.zeros((20, feature_count), dtype=np.float32),\n",
    "            'ones': np.ones((20, feature_count), dtype=np.float32),\n",
    "            'extremes': np.array([[-10]*feature_count, [10]*feature_count] * 10, dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        return test_data\n",
    "    \n",
    "    def generate_test_cases(self, model_type=None):\n",
    "        \"\"\"Generate appropriate test cases based on model type\"\"\"\n",
    "        if model_type is None:\n",
    "            model_type = self.model_type\n",
    "        \n",
    "        if model_type in ['text_classification', 'classification']:\n",
    "            return self.generate_text_classification_tests()\n",
    "        elif model_type == 'image_classification':\n",
    "            return self.generate_image_classification_tests()\n",
    "        elif model_type == 'regression':\n",
    "            return self.generate_regression_tests()\n",
    "        else:\n",
    "            # Default to text classification for unknown types\n",
    "            print(\"‚ö†Ô∏è  Unknown model type, defaulting to text classification tests\")\n",
    "            return self.generate_text_classification_tests()\n",
    "\n",
    "# Initialize test case generator\n",
    "if model_exists:\n",
    "    test_generator = TestCaseGenerator(analyzer.model_info['detected_type'])\n",
    "    test_cases = test_generator.generate_test_cases()\n",
    "    \n",
    "    print(\"üß™ Test cases generated successfully!\")\n",
    "    if isinstance(test_cases, dict):\n",
    "        for category, cases in test_cases.items():\n",
    "            if isinstance(cases, (list, np.ndarray)):\n",
    "                print(f\"  ‚Ä¢ {category}: {len(cases)} test cases\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping test case generation - model not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c019c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Testing Functions\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, analyzer, preprocessor=None):\n",
    "        \"\"\"Initialize model evaluator\"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        self.preprocessor = preprocessor\n",
    "        self.results = {}\n",
    "        \n",
    "    def predict_single(self, input_data):\n",
    "        \"\"\"Predict on a single input\"\"\"\n",
    "        if isinstance(input_data, str) and self.preprocessor:\n",
    "            # Text input - preprocess first\n",
    "            processed_input = self.preprocessor.preprocess_text(input_data)\n",
    "        elif isinstance(input_data, np.ndarray):\n",
    "            # Already preprocessed\n",
    "            processed_input = input_data\n",
    "        else:\n",
    "            raise ValueError(\"Input must be string (for text) or numpy array\")\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = self.analyzer.predict(processed_input)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def evaluate_text_classification(self, test_cases):\n",
    "        \"\"\"Evaluate text classification model on test cases\"\"\"\n",
    "        print(\"üîç EVALUATING TEXT CLASSIFICATION MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        all_results = {}\n",
    "        total_samples = 0\n",
    "        \n",
    "        for category, messages in test_cases.items():\n",
    "            if not isinstance(messages, list):\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüìã Testing {category} ({len(messages)} samples):\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            results = []\n",
    "            for i, message in enumerate(messages, 1):\n",
    "                try:\n",
    "                    prediction = self.predict_single(message)\n",
    "                    \n",
    "                    # Extract prediction details\n",
    "                    if len(prediction.shape) == 2 and prediction.shape[1] > 1:\n",
    "                        # Multi-class classification\n",
    "                        predicted_class = np.argmax(prediction[0])\n",
    "                        confidence = float(prediction[0][predicted_class])\n",
    "                        probabilities = prediction[0].tolist()\n",
    "                    else:\n",
    "                        # Binary classification or regression\n",
    "                        predicted_class = int(prediction[0] > MODEL_CONFIG['confidence_threshold'])\n",
    "                        confidence = float(prediction[0])\n",
    "                        probabilities = [1-confidence, confidence] if predicted_class else [confidence, 1-confidence]\n",
    "                    \n",
    "                    pred_label = \"Suspicious\" if predicted_class == 1 else \"Safe\"\n",
    "                    \n",
    "                    result = {\n",
    "                        'message': message,\n",
    "                        'prediction': pred_label,\n",
    "                        'predicted_class': predicted_class,\n",
    "                        'confidence': confidence,\n",
    "                        'probabilities': probabilities\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Print sample results\n",
    "                    message_preview = message[:50] + \"...\" if len(message) > 50 else message\n",
    "                    print(f\"  {i:2d}. '{message_preview}'\")\n",
    "                    print(f\"      ‚Üí {pred_label} (confidence: {confidence:.3f})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  {i:2d}. Error processing message: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            all_results[category] = results\n",
    "            total_samples += len(results)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation completed! Processed {total_samples} samples\")\n",
    "        return all_results\n",
    "    \n",
    "    def evaluate_image_classification(self, test_images):\n",
    "        \"\"\"Evaluate image classification model on test images\"\"\"\n",
    "        print(\"üîç EVALUATING IMAGE CLASSIFICATION MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for category, images in test_images.items():\n",
    "            print(f\"\\nüìã Testing {category} ({len(images)} samples):\")\n",
    "            \n",
    "            results = []\n",
    "            for i, image in enumerate(images):\n",
    "                try:\n",
    "                    prediction = self.analyzer.predict(image.reshape(1, *image.shape))\n",
    "                    \n",
    "                    predicted_class = np.argmax(prediction[0])\n",
    "                    confidence = float(prediction[0][predicted_class])\n",
    "                    \n",
    "                    result = {\n",
    "                        'image_index': i,\n",
    "                        'predicted_class': predicted_class,\n",
    "                        'confidence': confidence,\n",
    "                        'probabilities': prediction[0].tolist()\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"  {i+1:2d}. Class: {predicted_class}, Confidence: {confidence:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  {i+1:2d}. Error: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            all_results[category] = results\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def evaluate_regression(self, test_data):\n",
    "        \"\"\"Evaluate regression model on test data\"\"\"\n",
    "        print(\"üîç EVALUATING REGRESSION MODEL\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for category, data in test_data.items():\n",
    "            print(f\"\\nüìã Testing {category} ({len(data)} samples):\")\n",
    "            \n",
    "            results = []\n",
    "            predictions = []\n",
    "            \n",
    "            for i, sample in enumerate(data):\n",
    "                try:\n",
    "                    prediction = self.analyzer.predict(sample.reshape(1, -1))\n",
    "                    pred_value = float(prediction[0])\n",
    "                    \n",
    "                    results.append({\n",
    "                        'sample_index': i,\n",
    "                        'prediction': pred_value\n",
    "                    })\n",
    "                    predictions.append(pred_value)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Sample {i+1}: Error - {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if predictions:\n",
    "                print(f\"  Mean prediction: {np.mean(predictions):.4f}\")\n",
    "                print(f\"  Std deviation: {np.std(predictions):.4f}\")\n",
    "                print(f\"  Min/Max: {np.min(predictions):.4f} / {np.max(predictions):.4f}\")\n",
    "            \n",
    "            all_results[category] = results\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_accuracy_metrics(self, results, expected_labels=None):\n",
    "        \"\"\"Calculate accuracy metrics for classification results\"\"\"\n",
    "        if not expected_labels:\n",
    "            # Default expected labels for text classification\n",
    "            expected_labels = {\n",
    "                'phishing_suspicious': 1,\n",
    "                'legitimate_safe': 0,\n",
    "                'mixed_scenarios': None,  # No specific expectation\n",
    "                'edge_cases': None\n",
    "            }\n",
    "        \n",
    "        print(\"\\nüìä ACCURACY ANALYSIS\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        total_correct = 0\n",
    "        total_tested = 0\n",
    "        \n",
    "        for category, category_results in results.items():\n",
    "            if category not in expected_labels or expected_labels[category] is None:\n",
    "                continue\n",
    "            \n",
    "            expected = expected_labels[category]\n",
    "            correct = sum(1 for r in category_results if r['predicted_class'] == expected)\n",
    "            total = len(category_results)\n",
    "            accuracy = correct / total if total > 0 else 0\n",
    "            \n",
    "            print(f\"{category}: {correct}/{total} = {accuracy:.2%}\")\n",
    "            \n",
    "            total_correct += correct\n",
    "            total_tested += total\n",
    "            \n",
    "            # Show misclassified examples\n",
    "            misclassified = [r for r in category_results if r['predicted_class'] != expected]\n",
    "            if misclassified and len(misclassified) <= 3:\n",
    "                print(f\"  Misclassified:\")\n",
    "                for item in misclassified:\n",
    "                    msg_preview = item['message'][:40] + \"...\" if len(item['message']) > 40 else item['message']\n",
    "                    print(f\"    - '{msg_preview}' ‚Üí {item['prediction']}\")\n",
    "        \n",
    "        overall_accuracy = total_correct / total_tested if total_tested > 0 else 0\n",
    "        print(f\"\\nüéØ OVERALL ACCURACY: {total_correct}/{total_tested} = {overall_accuracy:.2%}\")\n",
    "        \n",
    "        return overall_accuracy\n",
    "\n",
    "# Run evaluation if model is loaded\n",
    "if model_exists and 'test_cases' in locals():\n",
    "    evaluator = ModelEvaluator(analyzer, text_preprocessor)\n",
    "    \n",
    "    # Run appropriate evaluation based on model type\n",
    "    if analyzer.model_info['detected_type'] in ['text_classification', 'classification']:\n",
    "        evaluation_results = evaluator.evaluate_text_classification(test_cases)\n",
    "        accuracy = evaluator.calculate_accuracy_metrics(evaluation_results)\n",
    "    elif analyzer.model_info['detected_type'] == 'image_classification':\n",
    "        evaluation_results = evaluator.evaluate_image_classification(test_cases)\n",
    "    elif analyzer.model_info['detected_type'] == 'regression':\n",
    "        evaluation_results = evaluator.evaluate_regression(test_cases)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Model type not recognized for evaluation\")\n",
    "        evaluation_results = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping evaluation - model or test cases not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9900c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Benchmarking and Visualization\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, analyzer, preprocessor=None):\n",
    "        \"\"\"Initialize performance benchmark\"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "    def benchmark_inference_speed(self, num_iterations=100):\n",
    "        \"\"\"Benchmark model inference speed\"\"\"\n",
    "        print(\"‚è±Ô∏è  PERFORMANCE BENCHMARKING\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Prepare test input based on model type\n",
    "        input_shape = self.analyzer.input_details[0]['shape']\n",
    "        if self.analyzer.model_info['detected_type'] in ['text_classification', 'classification']:\n",
    "            # Use sample text\n",
    "            test_input = self.preprocessor.preprocess_text(\"This is a test message for benchmarking\")\n",
    "        else:\n",
    "            # Use random data matching input shape\n",
    "            test_input = np.random.rand(*input_shape[1:]).astype(np.float32)\n",
    "            test_input = np.expand_dims(test_input, axis=0)\n",
    "        \n",
    "        # Warm-up runs\n",
    "        for _ in range(5):\n",
    "            _ = self.analyzer.predict(test_input)\n",
    "        \n",
    "        # Benchmark runs\n",
    "        times = []\n",
    "        for _ in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            _ = self.analyzer.predict(test_input)\n",
    "            end_time = time.time()\n",
    "            times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        min_time = np.min(times)\n",
    "        max_time = np.max(times)\n",
    "        \n",
    "        print(f\"üìä Inference Speed Results ({num_iterations} iterations):\")\n",
    "        print(f\"  ‚Ä¢ Mean: {mean_time:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Std Dev: {std_time:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Min: {min_time:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Max: {max_time:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Throughput: {1000/mean_time:.1f} inferences/second\")\n",
    "        \n",
    "        return {\n",
    "            'times': times,\n",
    "            'mean': mean_time,\n",
    "            'std': std_time,\n",
    "            'min': min_time,\n",
    "            'max': max_time,\n",
    "            'throughput': 1000/mean_time\n",
    "        }\n",
    "    \n",
    "    def analyze_confidence_distribution(self, evaluation_results):\n",
    "        \"\"\"Analyze confidence score distribution\"\"\"\n",
    "        if not evaluation_results:\n",
    "            print(\"‚ö†Ô∏è  No evaluation results available for confidence analysis\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüìà CONFIDENCE SCORE ANALYSIS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        all_confidences = []\n",
    "        category_confidences = {}\n",
    "        \n",
    "        for category, results in evaluation_results.items():\n",
    "            if not isinstance(results, list):\n",
    "                continue\n",
    "                \n",
    "            confidences = [r['confidence'] for r in results if 'confidence' in r]\n",
    "            if confidences:\n",
    "                category_confidences[category] = confidences\n",
    "                all_confidences.extend(confidences)\n",
    "                \n",
    "                print(f\"\\n{category}:\")\n",
    "                print(f\"  ‚Ä¢ Mean confidence: {np.mean(confidences):.3f}\")\n",
    "                print(f\"  ‚Ä¢ Std deviation: {np.std(confidences):.3f}\")\n",
    "                print(f\"  ‚Ä¢ Min/Max: {np.min(confidences):.3f} / {np.max(confidences):.3f}\")\n",
    "        \n",
    "        if all_confidences:\n",
    "            print(f\"\\nOverall:\")\n",
    "            print(f\"  ‚Ä¢ Mean confidence: {np.mean(all_confidences):.3f}\")\n",
    "            print(f\"  ‚Ä¢ Std deviation: {np.std(all_confidences):.3f}\")\n",
    "        \n",
    "        return category_confidences\n",
    "    \n",
    "    def create_visualizations(self, evaluation_results, benchmark_results=None):\n",
    "        \"\"\"Create visualizations for model performance\"\"\"\n",
    "        if not evaluation_results:\n",
    "            print(\"‚ö†Ô∏è  No evaluation results available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plotting area\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('TensorFlow Lite Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Confidence Score Distribution\n",
    "        all_confidences = []\n",
    "        labels = []\n",
    "        for category, results in evaluation_results.items():\n",
    "            if isinstance(results, list) and results:\n",
    "                confidences = [r['confidence'] for r in results if 'confidence' in r]\n",
    "                all_confidences.extend(confidences)\n",
    "                labels.extend([category] * len(confidences))\n",
    "        \n",
    "        if all_confidences:\n",
    "            axes[0, 0].hist(all_confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[0, 0].set_title('Confidence Score Distribution')\n",
    "            axes[0, 0].set_xlabel('Confidence Score')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Predictions by Category\n",
    "        category_counts = {}\n",
    "        for category, results in evaluation_results.items():\n",
    "            if isinstance(results, list):\n",
    "                predictions = [r.get('prediction', 'Unknown') for r in results]\n",
    "                pred_counts = pd.Series(predictions).value_counts()\n",
    "                category_counts[category] = pred_counts\n",
    "        \n",
    "        if category_counts:\n",
    "            # Create stacked bar chart\n",
    "            categories = list(category_counts.keys())\n",
    "            all_pred_types = set()\n",
    "            for counts in category_counts.values():\n",
    "                all_pred_types.update(counts.index)\n",
    "            \n",
    "            bottom = np.zeros(len(categories))\n",
    "            colors = ['lightcoral', 'lightgreen', 'lightblue', 'lightyellow']\n",
    "            \n",
    "            for i, pred_type in enumerate(all_pred_types):\n",
    "                values = [category_counts[cat].get(pred_type, 0) for cat in categories]\n",
    "                axes[0, 1].bar(categories, values, bottom=bottom, \n",
    "                              label=pred_type, color=colors[i % len(colors)])\n",
    "                bottom += values\n",
    "            \n",
    "            axes[0, 1].set_title('Predictions by Category')\n",
    "            axes[0, 1].set_ylabel('Count')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 3. Inference Time Distribution (if benchmark results available)\n",
    "        if benchmark_results and 'times' in benchmark_results:\n",
    "            axes[1, 0].hist(benchmark_results['times'], bins=20, alpha=0.7, \n",
    "                           color='lightcoral', edgecolor='black')\n",
    "            axes[1, 0].set_title('Inference Time Distribution')\n",
    "            axes[1, 0].set_xlabel('Time (ms)')\n",
    "            axes[1, 0].set_ylabel('Frequency')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add mean line\n",
    "            mean_time = benchmark_results['mean']\n",
    "            axes[1, 0].axvline(mean_time, color='red', linestyle='--', \n",
    "                              label=f'Mean: {mean_time:.2f}ms')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # 4. Model Information Summary\n",
    "        axes[1, 1].axis('off')\n",
    "        model_info_text = f\"\"\"\n",
    "Model Information:\n",
    "‚Ä¢ Model Size: {self.analyzer.model_info['model_size_mb']:.2f} MB\n",
    "‚Ä¢ Model Type: {self.analyzer.model_info['detected_type']}\n",
    "‚Ä¢ Input Shape: {self.analyzer.model_info['inputs'][0]['shape']}\n",
    "‚Ä¢ Output Shape: {self.analyzer.model_info['outputs'][0]['shape']}\n",
    "\"\"\"\n",
    "        \n",
    "        if benchmark_results:\n",
    "            model_info_text += f\"\"\"\n",
    "Performance Metrics:\n",
    "‚Ä¢ Mean Inference: {benchmark_results['mean']:.2f} ms\n",
    "‚Ä¢ Throughput: {benchmark_results['throughput']:.1f} /sec\n",
    "‚Ä¢ Min/Max: {benchmark_results['min']:.2f}/{benchmark_results['max']:.2f} ms\n",
    "\"\"\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.9, model_info_text, transform=axes[1, 1].transAxes,\n",
    "                        fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run performance benchmarking and visualization\n",
    "if model_exists and 'evaluator' in locals():\n",
    "    benchmark = PerformanceBenchmark(analyzer, text_preprocessor)\n",
    "    \n",
    "    # Run speed benchmark\n",
    "    benchmark_results = benchmark.benchmark_inference_speed(MODEL_CONFIG['benchmark_iterations'])\n",
    "    \n",
    "    # Analyze confidence distribution\n",
    "    if 'evaluation_results' in locals():\n",
    "        confidence_analysis = benchmark.analyze_confidence_distribution(evaluation_results)\n",
    "        \n",
    "        # Create visualizations\n",
    "        benchmark.create_visualizations(evaluation_results, benchmark_results)\n",
    "    \n",
    "    print(\"\\nüéâ Performance analysis completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping performance benchmarking - model not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Testing and Custom Input Evaluation\n",
    "class InteractiveTester:\n",
    "    def __init__(self, analyzer, preprocessor=None):\n",
    "        \"\"\"Initialize interactive tester\"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "    def test_custom_input(self, input_data, input_type='auto'):\n",
    "        \"\"\"Test a single custom input with detailed analysis\"\"\"\n",
    "        print(\"üîç CUSTOM INPUT ANALYSIS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if input_type == 'auto':\n",
    "            # Auto-detect input type\n",
    "            if isinstance(input_data, str):\n",
    "                input_type = 'text'\n",
    "            elif isinstance(input_data, np.ndarray):\n",
    "                input_type = 'array'\n",
    "            else:\n",
    "                print(\"‚ùå Unsupported input type\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"üì• Input Type: {input_type}\")\n",
    "        \n",
    "        if input_type == 'text':\n",
    "            print(f\"üìù Original Text: '{input_data}'\")\n",
    "            \n",
    "            if self.preprocessor:\n",
    "                # Show preprocessing steps\n",
    "                cleaned = self.preprocessor.clean_text(input_data)\n",
    "                print(f\"üßπ Cleaned Text: '{cleaned}'\")\n",
    "                \n",
    "                processed = self.preprocessor.preprocess_text(input_data)\n",
    "                print(f\"üî¢ Processed Shape: {processed.shape}\")\n",
    "                \n",
    "                # Show tokenization if available\n",
    "                if self.preprocessor.tokenizer and hasattr(self.preprocessor.tokenizer, 'texts_to_sequences'):\n",
    "                    sequence = self.preprocessor.tokenizer.texts_to_sequences([cleaned])\n",
    "                    if sequence[0]:\n",
    "                        tokens = [self.preprocessor.tokenizer.index_word.get(idx, f'<UNK:{idx}>') \n",
    "                                for idx in sequence[0][:10]]  # Show first 10 tokens\n",
    "                        print(f\"üéØ Tokens: {tokens}...\")\n",
    "                \n",
    "                input_for_model = processed\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No preprocessor available - using raw input\")\n",
    "                input_for_model = np.array([[ord(c) for c in input_data[:100]]]).astype(np.float32)\n",
    "        \n",
    "        elif input_type == 'array':\n",
    "            print(f\"üìä Array Shape: {input_data.shape}\")\n",
    "            print(f\"üìä Array Type: {input_data.dtype}\")\n",
    "            print(f\"üìä Value Range: [{np.min(input_data):.3f}, {np.max(input_data):.3f}]\")\n",
    "            input_for_model = input_data\n",
    "        \n",
    "        # Get prediction\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            prediction = self.analyzer.predict(input_for_model)\n",
    "            inference_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è  Inference Time: {inference_time:.2f} ms\")\n",
    "            print(f\"üì§ Raw Output Shape: {prediction.shape}\")\n",
    "            print(f\"üì§ Raw Output: {prediction}\")\n",
    "            \n",
    "            # Interpret prediction based on model type\n",
    "            if self.analyzer.model_info['detected_type'] in ['text_classification', 'classification']:\n",
    "                if len(prediction.shape) == 2 and prediction.shape[1] > 1:\n",
    "                    # Multi-class classification\n",
    "                    predicted_class = np.argmax(prediction[0])\n",
    "                    confidence = float(prediction[0][predicted_class])\n",
    "                    probabilities = prediction[0]\n",
    "                    \n",
    "                    print(f\"\\nüéØ Predicted Class: {predicted_class}\")\n",
    "                    print(f\"üéØ Confidence: {confidence:.3f}\")\n",
    "                    print(f\"üéØ All Probabilities: {probabilities}\")\n",
    "                    \n",
    "                    # For binary classification, interpret as Safe/Suspicious\n",
    "                    if prediction.shape[1] == 2:\n",
    "                        label = \"Suspicious\" if predicted_class == 1 else \"Safe\"\n",
    "                        print(f\"üè∑Ô∏è  Interpretation: {label}\")\n",
    "                        \n",
    "                        # Risk assessment\n",
    "                        if predicted_class == 1:\n",
    "                            if confidence > 0.8:\n",
    "                                risk_level = \"üî¥ HIGH RISK\"\n",
    "                            elif confidence > 0.6:\n",
    "                                risk_level = \"üü° MEDIUM RISK\"\n",
    "                            else:\n",
    "                                risk_level = \"üü† LOW RISK\"\n",
    "                        else:\n",
    "                            risk_level = \"üü¢ SAFE\"\n",
    "                        \n",
    "                        print(f\"üö® Risk Level: {risk_level}\")\n",
    "                \n",
    "                else:\n",
    "                    # Binary output\n",
    "                    confidence = float(prediction[0])\n",
    "                    predicted_class = int(confidence > MODEL_CONFIG['confidence_threshold'])\n",
    "                    label = \"Suspicious\" if predicted_class == 1 else \"Safe\"\n",
    "                    \n",
    "                    print(f\"\\nüéØ Prediction Score: {confidence:.3f}\")\n",
    "                    print(f\"üéØ Predicted Class: {predicted_class} ({label})\")\n",
    "            \n",
    "            elif self.analyzer.model_info['detected_type'] == 'regression':\n",
    "                predicted_value = float(prediction[0])\n",
    "                print(f\"\\nüéØ Predicted Value: {predicted_value:.4f}\")\n",
    "            \n",
    "            else:\n",
    "                print(f\"\\nüéØ Raw Prediction: {prediction}\")\n",
    "            \n",
    "            return {\n",
    "                'prediction': prediction,\n",
    "                'inference_time': inference_time,\n",
    "                'processed_input': input_for_model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during prediction: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def batch_test_custom_inputs(self, inputs, input_type='auto'):\n",
    "        \"\"\"Test multiple custom inputs\"\"\"\n",
    "        print(\"üîç BATCH CUSTOM INPUT TESTING\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        results = []\n",
    "        for i, input_data in enumerate(inputs, 1):\n",
    "            print(f\"\\n--- Test {i}/{len(inputs)} ---\")\n",
    "            result = self.test_custom_input(input_data, input_type)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        if results:\n",
    "            # Summary statistics\n",
    "            inference_times = [r['inference_time'] for r in results]\n",
    "            print(f\"\\nüìä BATCH SUMMARY:\")\n",
    "            print(f\"  ‚Ä¢ Total Tests: {len(results)}\")\n",
    "            print(f\"  ‚Ä¢ Mean Inference Time: {np.mean(inference_times):.2f} ms\")\n",
    "            print(f\"  ‚Ä¢ Total Time: {sum(inference_times):.2f} ms\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Interactive Testing Examples\n",
    "if model_exists and 'evaluator' in locals():\n",
    "    interactive_tester = InteractiveTester(analyzer, text_preprocessor)\n",
    "    \n",
    "    # Test some custom examples\n",
    "    custom_test_messages = [\n",
    "        \"URGENT: Click this link to verify your account or it will be suspended!\",\n",
    "        \"Hey, are you available for dinner tonight?\",\n",
    "        \"Congratulations! You've won $5000! Call now to claim your prize!\",\n",
    "        \"Meeting has been moved to 3 PM in conference room B\",\n",
    "        \"\"  # Empty message test\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ TESTING CUSTOM MESSAGES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, message in enumerate(custom_test_messages, 1):\n",
    "        print(f\"\\n{'='*20} TEST {i} {'='*20}\")\n",
    "        interactive_tester.test_custom_input(message, 'text')\n",
    "    \n",
    "    print(\"\\n‚úÖ Interactive testing completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Interactive testing not available - model not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097365e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Advanced Analysis\n",
    "class ModelComparator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize model comparator\"\"\"\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def add_model(self, name, model_path, tokenizer_path=None):\n",
    "        \"\"\"Add a model for comparison\"\"\"\n",
    "        try:\n",
    "            analyzer = TFLiteModelAnalyzer(model_path)\n",
    "            preprocessor = None\n",
    "            \n",
    "            if tokenizer_path and analyzer.model_info['detected_type'] in ['text_classification', 'classification']:\n",
    "                preprocessor = TextPreprocessor(tokenizer_path)\n",
    "            \n",
    "            self.models[name] = {\n",
    "                'analyzer': analyzer,\n",
    "                'preprocessor': preprocessor,\n",
    "                'path': model_path\n",
    "            }\n",
    "            print(f\"‚úÖ Added model: {name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to add model {name}: {e}\")\n",
    "    \n",
    "    def compare_models(self, test_data, test_labels=None):\n",
    "        \"\"\"Compare all added models on the same test data\"\"\"\n",
    "        if not self.models:\n",
    "            print(\"‚ö†Ô∏è  No models to compare\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîç MODEL COMPARISON ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        comparison_results = {}\n",
    "        \n",
    "        for model_name, model_info in self.models.items():\n",
    "            print(f\"\\n--- Testing {model_name} ---\")\n",
    "            \n",
    "            analyzer = model_info['analyzer']\n",
    "            preprocessor = model_info['preprocessor']\n",
    "            \n",
    "            # Model info\n",
    "            print(f\"üìã Model Type: {analyzer.model_info['detected_type']}\")\n",
    "            print(f\"üìã Input Shape: {analyzer.model_info['inputs'][0]['shape']}\")\n",
    "            print(f\"üìã Output Shape: {analyzer.model_info['outputs'][0]['shape']}\")\n",
    "            \n",
    "            # Performance testing\n",
    "            predictions = []\n",
    "            inference_times = []\n",
    "            \n",
    "            for data in test_data:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                if preprocessor and isinstance(data, str):\n",
    "                    processed_data = preprocessor.preprocess_text(data)\n",
    "                else:\n",
    "                    processed_data = data\n",
    "                \n",
    "                pred = analyzer.predict(processed_data)\n",
    "                inference_time = (time.time() - start_time) * 1000\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                inference_times.append(inference_time)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_inference_time = np.mean(inference_times)\n",
    "            total_time = sum(inference_times)\n",
    "            \n",
    "            results = {\n",
    "                'predictions': predictions,\n",
    "                'avg_inference_time': avg_inference_time,\n",
    "                'total_time': total_time,\n",
    "                'model_info': analyzer.model_info.copy()\n",
    "            }\n",
    "            \n",
    "            # If labels are provided, calculate accuracy\n",
    "            if test_labels is not None:\n",
    "                try:\n",
    "                    # Convert predictions to binary classifications\n",
    "                    pred_classes = []\n",
    "                    for pred in predictions:\n",
    "                        if len(pred.shape) > 1 and pred.shape[1] > 1:\n",
    "                            pred_classes.append(np.argmax(pred))\n",
    "                        else:\n",
    "                            pred_classes.append(int(pred[0] > 0.5))\n",
    "                    \n",
    "                    accuracy = np.mean([p == l for p, l in zip(pred_classes, test_labels)])\n",
    "                    results['accuracy'] = accuracy\n",
    "                    print(f\"üéØ Accuracy: {accuracy:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Could not calculate accuracy: {e}\")\n",
    "            \n",
    "            print(f\"‚è±Ô∏è  Avg Inference Time: {avg_inference_time:.2f} ms\")\n",
    "            print(f\"‚è±Ô∏è  Total Time: {total_time:.2f} ms\")\n",
    "            \n",
    "            comparison_results[model_name] = results\n",
    "        \n",
    "        self.results = comparison_results\n",
    "        self._visualize_comparison()\n",
    "        \n",
    "        return comparison_results\n",
    "    \n",
    "    def _visualize_comparison(self):\n",
    "        \"\"\"Create visualization of model comparison\"\"\"\n",
    "        if not self.results:\n",
    "            return\n",
    "        \n",
    "        # Extract metrics for visualization\n",
    "        model_names = list(self.results.keys())\n",
    "        inference_times = [self.results[name]['avg_inference_time'] for name in model_names]\n",
    "        \n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Inference Time Comparison\n",
    "        axes[0, 0].bar(model_names, inference_times, color='skyblue')\n",
    "        axes[0, 0].set_title('Average Inference Time Comparison')\n",
    "        axes[0, 0].set_ylabel('Time (ms)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Model Size Comparison (if available)\n",
    "        model_sizes = []\n",
    "        for name in model_names:\n",
    "            try:\n",
    "                path = self.models[name]['path']\n",
    "                size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "                model_sizes.append(size_mb)\n",
    "            except:\n",
    "                model_sizes.append(0)\n",
    "        \n",
    "        if any(size > 0 for size in model_sizes):\n",
    "            axes[0, 1].bar(model_names, model_sizes, color='lightgreen')\n",
    "            axes[0, 1].set_title('Model Size Comparison')\n",
    "            axes[0, 1].set_ylabel('Size (MB)')\n",
    "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'Model sizes not available', \n",
    "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('Model Size Comparison')\n",
    "        \n",
    "        # 3. Accuracy Comparison (if available)\n",
    "        accuracies = []\n",
    "        for name in model_names:\n",
    "            acc = self.results[name].get('accuracy', None)\n",
    "            accuracies.append(acc if acc is not None else 0)\n",
    "        \n",
    "        if any(acc > 0 for acc in accuracies):\n",
    "            axes[1, 0].bar(model_names, accuracies, color='orange')\n",
    "            axes[1, 0].set_title('Accuracy Comparison')\n",
    "            axes[1, 0].set_ylabel('Accuracy')\n",
    "            axes[1, 0].set_ylim(0, 1)\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Accuracy data not available', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Accuracy Comparison')\n",
    "        \n",
    "        # 4. Performance vs Size Scatter Plot\n",
    "        valid_sizes = [s for s in model_sizes if s > 0]\n",
    "        valid_times = [t for t, s in zip(inference_times, model_sizes) if s > 0]\n",
    "        valid_names = [n for n, s in zip(model_names, model_sizes) if s > 0]\n",
    "        \n",
    "        if valid_sizes:\n",
    "            axes[1, 1].scatter(valid_sizes, valid_times, s=100, alpha=0.7)\n",
    "            for i, name in enumerate(valid_names):\n",
    "                axes[1, 1].annotate(name, (valid_sizes[i], valid_times[i]), \n",
    "                                   xytext=(5, 5), textcoords='offset points')\n",
    "            axes[1, 1].set_xlabel('Model Size (MB)')\n",
    "            axes[1, 1].set_ylabel('Inference Time (ms)')\n",
    "            axes[1, 1].set_title('Performance vs Size')\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Size vs Performance\\ndata not available', \n",
    "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Performance vs Size')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\nüìä COMPARISON SUMMARY TABLE\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Model':<20} {'Inf. Time (ms)':<15} {'Size (MB)':<12} {'Accuracy':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            inf_time = f\"{inference_times[i]:.2f}\"\n",
    "            size = f\"{model_sizes[i]:.2f}\" if model_sizes[i] > 0 else \"N/A\"\n",
    "            acc = f\"{accuracies[i]:.3f}\" if accuracies[i] > 0 else \"N/A\"\n",
    "            print(f\"{name:<20} {inf_time:<15} {size:<12} {acc:<10}\")\n",
    "\n",
    "# Advanced Model Analysis\n",
    "class AdvancedAnalyzer:\n",
    "    def __init__(self, analyzer):\n",
    "        \"\"\"Initialize advanced analyzer\"\"\"\n",
    "        self.analyzer = analyzer\n",
    "        \n",
    "    def analyze_sensitivity(self, base_input, preprocessor=None, variations=10):\n",
    "        \"\"\"Analyze model sensitivity to input variations\"\"\"\n",
    "        print(\"üî¨ MODEL SENSITIVITY ANALYSIS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if preprocessor and isinstance(base_input, str):\n",
    "            processed_base = preprocessor.preprocess_text(base_input)\n",
    "        else:\n",
    "            processed_base = base_input\n",
    "        \n",
    "        base_prediction = self.analyzer.predict(processed_base)\n",
    "        base_confidence = float(base_prediction[0])\n",
    "        \n",
    "        print(f\"üìù Base Input: '{base_input}' (Confidence: {base_confidence:.3f})\")\n",
    "        \n",
    "        # Generate variations for text input\n",
    "        if isinstance(base_input, str):\n",
    "            variations_list = self._generate_text_variations(base_input, variations)\n",
    "        else:\n",
    "            variations_list = self._generate_array_variations(processed_base, variations)\n",
    "        \n",
    "        sensitivity_results = []\n",
    "        \n",
    "        for i, variation in enumerate(variations_list):\n",
    "            try:\n",
    "                if preprocessor and isinstance(variation, str):\n",
    "                    processed_var = preprocessor.preprocess_text(variation)\n",
    "                else:\n",
    "                    processed_var = variation\n",
    "                \n",
    "                var_prediction = self.analyzer.predict(processed_var)\n",
    "                var_confidence = float(var_prediction[0])\n",
    "                confidence_change = abs(var_confidence - base_confidence)\n",
    "                \n",
    "                sensitivity_results.append({\n",
    "                    'variation': variation,\n",
    "                    'confidence': var_confidence,\n",
    "                    'change': confidence_change\n",
    "                })\n",
    "                \n",
    "                print(f\"  Variation {i+1}: {confidence_change:.3f} change\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Variation {i+1}: Error - {e}\")\n",
    "        \n",
    "        # Analyze sensitivity\n",
    "        if sensitivity_results:\n",
    "            changes = [r['change'] for r in sensitivity_results]\n",
    "            avg_sensitivity = np.mean(changes)\n",
    "            max_sensitivity = np.max(changes)\n",
    "            \n",
    "            print(f\"\\nüìä SENSITIVITY METRICS:\")\n",
    "            print(f\"  ‚Ä¢ Average Sensitivity: {avg_sensitivity:.4f}\")\n",
    "            print(f\"  ‚Ä¢ Maximum Sensitivity: {max_sensitivity:.4f}\")\n",
    "            print(f\"  ‚Ä¢ Stability Rating: {'High' if avg_sensitivity < 0.1 else 'Medium' if avg_sensitivity < 0.3 else 'Low'}\")\n",
    "        \n",
    "        return sensitivity_results\n",
    "    \n",
    "    def _generate_text_variations(self, text, count):\n",
    "        \"\"\"Generate text variations for sensitivity testing\"\"\"\n",
    "        variations = []\n",
    "        \n",
    "        # Add spaces\n",
    "        variations.append(text + \" \")\n",
    "        variations.append(\" \" + text)\n",
    "        \n",
    "        # Case variations\n",
    "        variations.append(text.upper())\n",
    "        variations.append(text.lower())\n",
    "        variations.append(text.title())\n",
    "        \n",
    "        # Punctuation variations\n",
    "        variations.append(text + \"!\")\n",
    "        variations.append(text + \"?\")\n",
    "        variations.append(text.replace(\".\", \"\"))\n",
    "        \n",
    "        # Word order (simple)\n",
    "        words = text.split()\n",
    "        if len(words) > 1:\n",
    "            variations.append(\" \".join(words[::-1]))  # Reverse order\n",
    "        \n",
    "        return variations[:count]\n",
    "    \n",
    "    def _generate_array_variations(self, array, count):\n",
    "        \"\"\"Generate array variations for sensitivity testing\"\"\"\n",
    "        variations = []\n",
    "        \n",
    "        for i in range(count):\n",
    "            # Add small random noise\n",
    "            noise_level = 0.01 * (i + 1)\n",
    "            noisy = array + np.random.normal(0, noise_level, array.shape).astype(array.dtype)\n",
    "            variations.append(np.clip(noisy, array.min(), array.max()))\n",
    "        \n",
    "        return variations\n",
    "\n",
    "# Initialize advanced tools if model is available\n",
    "if model_exists and 'analyzer' in locals():\n",
    "    print(\"üîß ADVANCED ANALYSIS TOOLS READY\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create advanced analyzer\n",
    "    advanced_analyzer = AdvancedAnalyzer(analyzer)\n",
    "    \n",
    "    # Create model comparator\n",
    "    model_comparator = ModelComparator()\n",
    "    \n",
    "    # Add current model to comparator\n",
    "    model_comparator.add_model(\"Current Model\", MODEL_CONFIG['tflite_model_path'], \n",
    "                              MODEL_CONFIG.get('tokenizer_path'))\n",
    "    \n",
    "    print(\"‚úÖ Advanced analysis tools initialized!\")\n",
    "    print(\"\\nAvailable tools:\")\n",
    "    print(\"‚Ä¢ interactive_tester - Test custom inputs\")\n",
    "    print(\"‚Ä¢ advanced_analyzer - Sensitivity analysis\")\n",
    "    print(\"‚Ä¢ model_comparator - Compare multiple models\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Advanced analysis tools not available - model not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0703321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Usage Instructions\n",
    "print(\"üéØ TFLITE MODEL EVALUATOR - READY FOR USE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if model_exists:\n",
    "    print(\"‚úÖ MODEL SUCCESSFULLY LOADED AND ANALYZED\")\n",
    "    print(f\"   Model Path: {MODEL_CONFIG['tflite_model_path']}\")\n",
    "    if MODEL_CONFIG.get('tokenizer_path'):\n",
    "        print(f\"   Tokenizer Path: {MODEL_CONFIG['tokenizer_path']}\")\n",
    "    print(f\"   Model Type: {analyzer.model_info['detected_type']}\")\n",
    "    print(f\"   Input Shape: {analyzer.model_info['inputs'][0]['shape']}\")\n",
    "    print(f\"   Output Shape: {analyzer.model_info['outputs'][0]['shape']}\")\n",
    "    \n",
    "    print(\"\\nüîß AVAILABLE TOOLS:\")\n",
    "    print(\"   ‚Ä¢ analyzer - Core model analysis and prediction\")\n",
    "    print(\"   ‚Ä¢ text_preprocessor - Text preprocessing (if applicable)\")\n",
    "    print(\"   ‚Ä¢ test_generator - Generate test cases\")\n",
    "    print(\"   ‚Ä¢ evaluator - Model evaluation and metrics\")\n",
    "    print(\"   ‚Ä¢ performance_benchmark - Performance analysis\")\n",
    "    print(\"   ‚Ä¢ interactive_tester - Interactive testing\")\n",
    "    print(\"   ‚Ä¢ advanced_analyzer - Sensitivity analysis\")\n",
    "    print(\"   ‚Ä¢ model_comparator - Multi-model comparison\")\n",
    "    \n",
    "    print(\"\\nüìã QUICK USAGE EXAMPLES:\")\n",
    "    print(\"   # Test a single message:\")\n",
    "    print(\"   interactive_tester.test_custom_input('Your test message here')\")\n",
    "    print()\n",
    "    print(\"   # Generate and test multiple cases:\")\n",
    "    print(\"   test_cases, labels = test_generator.generate_comprehensive_test_set(50)\")\n",
    "    print(\"   results = evaluator.evaluate_model(test_cases, labels)\")\n",
    "    print()\n",
    "    print(\"   # Benchmark performance:\")\n",
    "    print(\"   performance_benchmark.run_comprehensive_benchmark()\")\n",
    "    print()\n",
    "    print(\"   # Compare with another model:\")\n",
    "    print(\"   model_comparator.add_model('Model2', 'path/to/model2.tflite')\")\n",
    "    print(\"   model_comparator.compare_models(test_cases, labels)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  MODEL NOT LOADED\")\n",
    "    print(\"   Please update MODEL_CONFIG with valid paths and rerun the notebook\")\n",
    "    print()\n",
    "    print(\"üìù TO USE THIS NOTEBOOK:\")\n",
    "    print(\"   1. Update MODEL_CONFIG at the top with your model paths\")\n",
    "    print(\"   2. Run all cells in order\")\n",
    "    print(\"   3. Use the available tools for analysis\")\n",
    "\n",
    "print(\"\\nüöÄ READY TO ANALYZE YOUR TFLITE MODELS!\")\n",
    "print(\"   This notebook provides comprehensive evaluation capabilities\")\n",
    "print(\"   for any TensorFlow Lite model with automatic type detection.\")\n",
    "\n",
    "# Configuration reminder\n",
    "print(\"\\n‚öôÔ∏è  CURRENT CONFIGURATION:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    status = \"‚úÖ\" if (key == 'tflite_model_path' and os.path.exists(value)) or key != 'tflite_model_path' else \"‚ùå\"\n",
    "    print(f\"   {status} {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
