{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0fb42e-74e1-4b9d-a465-adec9c4e2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed252f94-0349-4cc2-9b48-a5bb136789d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading credit card fraud dataset...\n"
     ]
    },
    {
     "ename": "OpenMLError",
     "evalue": "Dataset credit-card-fraud with version 1 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenMLError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\JAVA\\CODE\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:329\u001b[39m, in \u001b[36m_get_data_info_by_name\u001b[39m\u001b[34m(name, version, data_home, n_retries, delay)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     json_data = \u001b[43m_get_json_content_from_openml_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_message\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenMLError:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# we can do this in 1 function call if OpenML does not require the\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# specification of the dataset status (i.e., return datasets with a\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# given name / version regardless of active, deactivated, etc. )\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# TODO: feature request OpenML.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\JAVA\\CODE\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:254\u001b[39m, in \u001b[36m_get_json_content_from_openml_api\u001b[39m\u001b[34m(url, error_message, data_home, n_retries, delay)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# 412 error, not in except for nicer traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenMLError(error_message)\n",
      "\u001b[31mOpenMLError\u001b[39m: None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenMLError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading credit card fraud dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the dataset from OpenML\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Note: This is the first load and might take some time\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cc_fraud = \u001b[43mfetch_openml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcredit-card-fraud\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_frame\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract features and target\u001b[39;00m\n\u001b[32m      8\u001b[39m X = cc_fraud.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\JAVA\\CODE\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\JAVA\\CODE\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1011\u001b[39m, in \u001b[36mfetch_openml\u001b[39m\u001b[34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[39m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1006\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1007\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDataset data_id=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and name=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m passed, but you can only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1008\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mspecify a numeric data_id or a name, not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1009\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mboth.\u001b[39m\u001b[33m\"\u001b[39m.format(data_id, name)\n\u001b[32m   1010\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m     data_info = \u001b[43m_get_data_info_by_name\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m     data_id = data_info[\u001b[33m\"\u001b[39m\u001b[33mdid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m data_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;66;03m# from the previous if statement, it is given that name is None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\JAVA\\CODE\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:343\u001b[39m, in \u001b[36m_get_data_info_by_name\u001b[39m\u001b[34m(name, version, data_home, n_retries, delay)\u001b[39m\n\u001b[32m    341\u001b[39m     url += \u001b[33m\"\u001b[39m\u001b[33m/status/deactivated\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     error_msg = \u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m with version \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not found.\u001b[39m\u001b[33m\"\u001b[39m.format(name, version)\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     json_data = \u001b[43m_get_json_content_from_openml_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m json_data[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\JAVA\\CODE\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:254\u001b[39m, in \u001b[36m_get_json_content_from_openml_api\u001b[39m\u001b[34m(url, error_message, data_home, n_retries, delay)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# 412 error, not in except for nicer traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenMLError(error_message)\n",
      "\u001b[31mOpenMLError\u001b[39m: Dataset credit-card-fraud with version 1 not found."
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Dataset\n",
    "print(\"Loading credit card fraud dataset...\")\n",
    "# Load the dataset from OpenML\n",
    "# Note: This is the first load and might take some time\n",
    "cc_fraud = fetch_openml(name='credit-card-fraud', version=1, as_frame=True)\n",
    "\n",
    "# Extract features and target\n",
    "X = cc_fraud.data\n",
    "y = cc_fraud.target.astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Fraud percentage: {y.mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c48c1-fee7-4f26-834d-43c50ee60635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Exploration\n",
    "# Create a DataFrame for easier exploration\n",
    "df = pd.DataFrame(X)\n",
    "df['Class'] = y\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Basic statistics of the dataset:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values found\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Credit Card Transactions - Class Distribution')\n",
    "plt.yscale('log')  # Using log scale for better visualization\n",
    "plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Add count labels on top of bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='Class', data=df)\n",
    "plt.title('Credit Card Transactions - Class Distribution')\n",
    "plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count and percentage labels\n",
    "total = len(df['Class'])\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2.,\n",
    "            height + 0.1,\n",
    "            f'{height}\\n({height/total:.4%})',\n",
    "            ha=\"center\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40b55a-7233-4425-8729-cdb008c538b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Visualization\n",
    "# Select a subset of features for visualization\n",
    "selected_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'Amount']\n",
    "\n",
    "# Pairplot for selected features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df[selected_features + ['Class']], hue='Class', diag_kind='hist')\n",
    "plt.suptitle('Pairplot of Selected Features', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Amount by class\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[df['Class'] == 0]['Amount'], bins=50, kde=True, color='blue')\n",
    "plt.title('Amount Distribution - Normal Transactions')\n",
    "plt.xlabel('Amount')\n",
    "plt.xlim([0, 500])  # Focus on common transaction amounts\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df[df['Class'] == 1]['Amount'], bins=50, kde=True, color='red')\n",
    "plt.title('Amount Distribution - Fraudulent Transactions')\n",
    "plt.xlabel('Amount')\n",
    "plt.xlim([0, 500])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix for selected features\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[selected_features + ['Class']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Selected Features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88138103-402c-40c5-b7a2-69007082448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Preprocessing\n",
    "# Split into features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data split summary:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Fraud ratio in training set: {y_train.mean():.4%}\")\n",
    "print(f\"Fraud ratio in testing set: {y_test.mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d06be4-4640-498f-a0d4-9bf0751dac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Handle Class Imbalance\n",
    "# Let's explore different approaches to handle class imbalance\n",
    "\n",
    "# 1. Original imbalanced data\n",
    "X_train_orig = X_train\n",
    "y_train_orig = y_train\n",
    "\n",
    "# 2. Random Undersampling\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.1, random_state=42)  # Keep 10% of majority class\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# 3. SMOTE Oversampling\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)  # Create synthetic samples until minority class is 10% of majority\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display class distribution for each approach\n",
    "print(\"Class distribution in different sampling approaches:\")\n",
    "print(f\"Original - Fraud ratio: {y_train_orig.mean():.4%}, Total samples: {len(y_train_orig)}\")\n",
    "print(f\"Undersampling - Fraud ratio: {y_train_under.mean():.4%}, Total samples: {len(y_train_under)}\")\n",
    "print(f\"SMOTE - Fraud ratio: {y_train_smote.mean():.4%}, Total samples: {len(y_train_smote)}\")\n",
    "\n",
    "# Visualize class distribution after sampling\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.countplot(x=y_train_orig)\n",
    "plt.title('Original Data')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Class')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.countplot(x=y_train_under)\n",
    "plt.title('After Undersampling')\n",
    "plt.xlabel('Class')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.countplot(x=y_train_smote)\n",
    "plt.title('After SMOTE')\n",
    "plt.xlabel('Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e3b1d-7b66-4824-a2f8-66f6993f7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Train Logistic Regression Models\n",
    "# We'll train multiple models with different approaches to imbalanced data\n",
    "\n",
    "# Original data model\n",
    "model_orig = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "model_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "# Undersampling model\n",
    "model_under = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_under.fit(X_train_under, y_train_under)\n",
    "\n",
    "# SMOTE model\n",
    "model_smote = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"Models trained successfully!\")\n",
    "\n",
    "# Check cross-validation scores\n",
    "print(\"\\nCross-validation results (F1 Score):\")\n",
    "cv_scores_orig = cross_val_score(model_orig, X_train_orig, y_train_orig, cv=5, scoring='f1')\n",
    "cv_scores_under = cross_val_score(model_under, X_train_under, y_train_under, cv=5, scoring='f1')\n",
    "cv_scores_smote = cross_val_score(model_smote, X_train_smote, y_train_smote, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"Original data model: {cv_scores_orig.mean():.4f} (±{cv_scores_orig.std():.4f})\")\n",
    "print(f\"Undersampling model: {cv_scores_under.mean():.4f} (±{cv_scores_under.std():.4f})\")\n",
    "print(f\"SMOTE model: {cv_scores_smote.mean():.4f} (±{cv_scores_smote.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f52e0e-10f8-4ae1-835c-c674335df441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model Evaluation\n",
    "# Make predictions\n",
    "y_pred_orig = model_orig.predict(X_test)\n",
    "y_pred_under = model_under.predict(X_test)\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "\n",
    "# Get probabilities for ROC curve\n",
    "y_prob_orig = model_orig.predict_proba(X_test)[:, 1]\n",
    "y_prob_under = model_under.predict_proba(X_test)[:, 1]\n",
    "y_prob_smote = model_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create a function to display evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, y_prob, model_name):\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Fraud'],\n",
    "                yticklabels=['Normal', 'Fraud'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "metrics_orig = evaluate_model(y_test, y_pred_orig, y_prob_orig, \"Original Model (with class_weight='balanced')\")\n",
    "metrics_under = evaluate_model(y_test, y_pred_under, y_prob_under, \"Undersampling Model\")\n",
    "metrics_smote = evaluate_model(y_test, y_pred_smote, y_prob_smote, \"SMOTE Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410b163-bcfe-4b12-8b5d-4f7222825c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: ROC and Precision-Recall Curves\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC curve subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "# Original model\n",
    "fpr_orig, tpr_orig, _ = roc_curve(y_test, y_prob_orig)\n",
    "roc_auc_orig = auc(fpr_orig, tpr_orig)\n",
    "plt.plot(fpr_orig, tpr_orig, label=f'Original (AUC = {roc_auc_orig:.4f})')\n",
    "\n",
    "# Undersampling model\n",
    "fpr_under, tpr_under, _ = roc_curve(y_test, y_prob_under)\n",
    "roc_auc_under = auc(fpr_under, tpr_under)\n",
    "plt.plot(fpr_under, tpr_under, label=f'Undersampling (AUC = {roc_auc_under:.4f})')\n",
    "\n",
    "# SMOTE model\n",
    "fpr_smote, tpr_smote, _ = roc_curve(y_test, y_prob_smote)\n",
    "roc_auc_smote = auc(fpr_smote, tpr_smote)\n",
    "plt.plot(fpr_smote, tpr_smote, label=f'SMOTE (AUC = {roc_auc_smote:.4f})')\n",
    "\n",
    "# Add diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall curve subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "# Original model\n",
    "precision_orig, recall_orig, _ = precision_recall_curve(y_test, y_prob_orig)\n",
    "pr_auc_orig = auc(recall_orig, precision_orig)\n",
    "plt.plot(recall_orig, precision_orig, label=f'Original (AUC = {pr_auc_orig:.4f})')\n",
    "\n",
    "# Undersampling model\n",
    "precision_under, recall_under, _ = precision_recall_curve(y_test, y_prob_under)\n",
    "pr_auc_under = auc(recall_under, precision_under)\n",
    "plt.plot(recall_under, precision_under, label=f'Undersampling (AUC = {pr_auc_under:.4f})')\n",
    "\n",
    "# SMOTE model\n",
    "precision_smote, recall_smote, _ = precision_recall_curve(y_test, y_prob_smote)\n",
    "pr_auc_smote = auc(recall_smote, precision_smote)\n",
    "plt.plot(recall_smote, precision_smote, label=f'SMOTE (AUC = {pr_auc_smote:.4f})')\n",
    "\n",
    "# Add baseline\n",
    "plt.axhline(y=y_test.mean(), color='r', linestyle='--', \n",
    "            label=f'Baseline ({y_test.mean():.4f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aada5c-250d-4a7c-ba44-8f2977b7dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Feature Importance Analysis\n",
    "# Analyze which features are most predictive of fraud\n",
    "\n",
    "# Get feature importance from coefficients (for SMOTE model)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model_smote.coef_[0]\n",
    "})\n",
    "feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display top 15 features\n",
    "print(\"Top 15 most important features:\")\n",
    "display(feature_importance.head(15))\n",
    "\n",
    "# Visualize top 10 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(10)\n",
    "sns.barplot(x='Coefficient', y='Feature', data=top_features, palette='viridis')\n",
    "plt.title('Top 10 Most Important Features for Fraud Detection')\n",
    "plt.xlabel('Coefficient Value (impact on log-odds)')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(True, axis='x')\n",
    "plt.show()\n",
    "\n",
    "# Show feature distributions for top 3 features\n",
    "top3_features = feature_importance['Feature'].head(3).tolist()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, feature in enumerate(top3_features):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.kdeplot(df[df['Class'] == 0][feature], label='Normal')\n",
    "    sns.kdeplot(df[df['Class'] == 1][feature], label='Fraud')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f307d07-7f2b-490e-9836-c02b438cfc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Threshold Analysis\n",
    "# Let's analyze how different probability thresholds affect model performance\n",
    "# We'll use the SMOTE model as it had good overall performance\n",
    "\n",
    "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "threshold_metrics = []\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    # Make predictions with the current threshold\n",
    "    y_pred_threshold = (y_prob_smote >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, y_pred_threshold)\n",
    "    recall = recall_score(y_test, y_pred_threshold)\n",
    "    f1 = f1_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    # Store metrics\n",
    "    threshold_metrics.append({\n",
    "        'Threshold': threshold,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_threshold)\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Normal', 'Fraud'],\n",
    "               yticklabels=['Normal', 'Fraud'])\n",
    "    plt.title(f'Threshold: {threshold}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display metrics table\n",
    "threshold_df = pd.DataFrame(threshold_metrics)\n",
    "display(threshold_df)\n",
    "\n",
    "# Plot metrics vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 'bo-', label='Precision')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['Recall'], 'ro-', label='Recall')\n",
    "plt.plot(threshold_df['Threshold'], threshold_df['F1 Score'], 'go-', label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics vs. Threshold')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69096d1-8ac4-4477-becc-58b7edd61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Final Model Selection and Conclusion\n",
    "# Compare all models\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Original (balanced)', 'Undersampling', 'SMOTE'],\n",
    "    'Accuracy': [metrics_orig[0], metrics_under[0], metrics_smote[0]],\n",
    "    'Precision': [metrics_orig[1], metrics_under[1], metrics_smote[1]],\n",
    "    'Recall': [metrics_orig[2], metrics_under[2], metrics_smote[2]],\n",
    "    'F1 Score': [metrics_orig[3], metrics_under[3], metrics_smote[3]],\n",
    "    'ROC AUC': [roc_auc_orig, roc_auc_under, roc_auc_smote],\n",
    "    'PR AUC': [pr_auc_orig, pr_auc_under, pr_auc_smote]\n",
    "})\n",
    "\n",
    "display(model_comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "metrics = ['Precision', 'Recall', 'F1 Score', 'ROC AUC', 'PR AUC']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.barplot(x='Model', y=metric, data=model_comparison)\n",
    "    plt.title(metric)\n",
    "    plt.ylim(0, 1)\n",
    "    for j, val in enumerate(model_comparison[metric]):\n",
    "        plt.text(j, val + 0.01, f'{val:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final conclusion\n",
    "print(\"\\n=== CONCLUSION ===\")\n",
    "print(\"Based on the evaluation metrics, here are the findings:\")\n",
    "\n",
    "# Identify best model by F1 score\n",
    "best_model_idx = model_comparison['F1 Score'].idxmax()\n",
    "best_model = model_comparison.loc[best_model_idx, 'Model']\n",
    "best_f1 = model_comparison.loc[best_model_idx, 'F1 Score']\n",
    "best_precision = model_comparison.loc[best_model_idx, 'Precision']\n",
    "best_recall = model_comparison.loc[best_model_idx, 'Recall']\n",
    "\n",
    "print(f\"The best performing model is: {best_model}\")\n",
    "print(f\"F1 Score: {best_f1:.4f}\")\n",
    "print(f\"Precision: {best_precision:.4f} (How many predicted frauds are actually fraud)\")\n",
    "print(f\"Recall: {best_recall:.4f} (What percentage of actual frauds were detected)\")\n",
    "print(\"\\nRecommendation: For fraud detection, high recall is usually more important than high precision.\")\n",
    "print(\"This is because the cost of missing a fraudulent transaction (false negative) is typically higher\")\n",
    "print(\"than the cost of investigating a legitimate transaction (false positive).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
