{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e95207",
   "metadata": {},
   "source": [
    "# Boston Housing Price Prediction System\n",
    "## Government of Massachusetts Real Estate Development Project\n",
    "\n",
    "**Objective**: Build a reliable prediction system to estimate median home values (MEDV) based on neighborhood characteristics to support housing policy decisions.\n",
    "\n",
    "**Dataset**: Boston Housing Dataset\n",
    "- Historical data from various neighborhoods\n",
    "- Features include crime rate, average rooms, population status, highway accessibility\n",
    "- Target: Median value of homes (MEDV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Setting up Boston Housing Price Prediction System...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2c660",
   "metadata": {},
   "source": [
    "## 1. Problem Implementation (5 Marks)\n",
    "\n",
    "### 1.1 Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1183e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset from CSV file\n",
    "data = pd.read_csv('/content/housing.csv')\n",
    "\n",
    "# Separate features and target\n",
    "# Assuming the last column is the target variable (MEDV)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "print(\"Boston Housing Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"Target variable: {y.name}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b327e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration\n",
    "print(\"=== DATASET EXPLORATION ===\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Number of features: {len(X.columns)}\")\n",
    "\n",
    "print(\"\\n=== DATASET INFO ===\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\n=== FEATURE DESCRIPTIONS ===\")\n",
    "feature_descriptions = {\n",
    "    'CRIM': 'Per capita crime rate by town',\n",
    "    'ZN': 'Proportion of residential land zoned for lots over 25,000 sq.ft.',\n",
    "    'INDUS': 'Proportion of non-retail business acres per town',\n",
    "    'CHAS': 'Charles River dummy variable (1 if tract bounds river; 0 otherwise)',\n",
    "    'NOX': 'Nitric oxides concentration (parts per 10 million)',\n",
    "    'RM': 'Average number of rooms per dwelling',\n",
    "    'AGE': 'Proportion of owner-occupied units built prior to 1940',\n",
    "    'DIS': 'Weighted distances to employment centres',\n",
    "    'RAD': 'Index of accessibility to radial highways',\n",
    "    'TAX': 'Full-value property-tax rate per $10,000',\n",
    "    'PTRATIO': 'Pupil-teacher ratio by town',\n",
    "    'B': '1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town',\n",
    "    'LSTAT': '% lower status of the population',\n",
    "    'MEDV': 'Median value of owner-occupied homes in $1000s (TARGET)'\n",
    "}\n",
    "\n",
    "for feature, description in feature_descriptions.items():\n",
    "    print(f\"{feature}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES CHECK ===\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\nGreat! No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(f\"\\nFound {missing_values.sum()} missing values that need to be handled.\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0c17b",
   "metadata": {},
   "source": [
    "### 1.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "print(\"=== DATA PREPROCESSING ===\")\n",
    "\n",
    "# Check if there are any missing values to handle\n",
    "total_missing = data.isnull().sum().sum()\n",
    "if total_missing > 0:\n",
    "    print(f\"Handling {total_missing} missing values...\")\n",
    "    # Fill missing values with median for numerical columns\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['float64', 'int64']:\n",
    "                data[column].fillna(data[column].median(), inplace=True)\n",
    "                print(f\"Filled missing values in {column} with median\")\n",
    "else:\n",
    "    print(\"No missing values to handle.\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # Last column as target\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "print(f\"Target variable: {y.name if hasattr(y, 'name') else 'MEDV'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"=== TRAIN-TEST SPLIT ===\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n=== FEATURE SCALING ===\")\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(\"Training set: fitted and transformed\")\n",
    "print(\"Testing set: transformed using training set parameters\")\n",
    "\n",
    "# Convert back to DataFrame for better readability (optional)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223275c9",
   "metadata": {},
   "source": [
    "### 1.3 Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest Regressor\n",
    "print(\"=== RANDOM FOREST MODEL BUILDING ===\")\n",
    "\n",
    "# Initialize Random Forest Regressor with optimal parameters\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees in the forest\n",
    "    max_depth=10,          # Maximum depth of the tree\n",
    "    min_samples_split=5,   # Minimum samples required to split a node\n",
    "    min_samples_leaf=3,    # Minimum samples required at a leaf node\n",
    "    random_state=42,       # For reproducibility\n",
    "    n_jobs=-1             # Use all available processors\n",
    ")\n",
    "\n",
    "print(\"Random Forest Regressor initialized with parameters:\")\n",
    "print(f\"- n_estimators: {rf_model.n_estimators}\")\n",
    "print(f\"- max_depth: {rf_model.max_depth}\")\n",
    "print(f\"- min_samples_split: {rf_model.min_samples_split}\")\n",
    "print(f\"- min_samples_leaf: {rf_model.min_samples_leaf}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the Random Forest model...\")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training completed!\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nTrained model details:\")\n",
    "print(f\"- Number of features used: {rf_model.n_features_}\")\n",
    "print(f\"- Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"- Model score on training data: {rf_model.score(X_train_scaled, y_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ae7c4",
   "metadata": {},
   "source": [
    "### 1.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation using R², MSE, and MAE\n",
    "print(\"=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Make predictions on both training and testing sets\n",
    "y_train_pred = rf_model.predict(X_train_scaled)\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics for training set\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "# Calculate evaluation metrics for testing set\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(\"TRAINING SET PERFORMANCE:\")\n",
    "print(f\"R² Score: {train_r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {train_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {train_rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {train_mae:.4f}\")\n",
    "\n",
    "print(\"\\nTESTING SET PERFORMANCE:\")\n",
    "print(f\"R² Score: {test_r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae:.4f}\")\n",
    "\n",
    "# Model interpretation\n",
    "print(\"\\n=== MODEL INTERPRETATION ===\")\n",
    "print(f\"The model explains {test_r2*100:.2f}% of the variance in housing prices.\")\n",
    "print(f\"On average, predictions are off by ${test_mae:.2f}k from actual prices.\")\n",
    "print(f\"The model performs {'well' if test_r2 > 0.8 else 'reasonably' if test_r2 > 0.6 else 'poorly'} on unseen data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82fd5c",
   "metadata": {},
   "source": [
    "## 2. Visualization and Inference (3 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting environment\n",
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 2.1 Histogram of the target variable (MEDV)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(y, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Housing Prices (MEDV)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Median Home Value ($1000s)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics to the plot\n",
    "mean_price = y.mean()\n",
    "std_price = y.std()\n",
    "plt.axvline(mean_price, color='red', linestyle='--', linewidth=2, label=f'Mean: ${mean_price:.1f}k')\n",
    "plt.axvline(mean_price + std_price, color='orange', linestyle='--', alpha=0.7, label=f'Mean + 1 STD: ${mean_price + std_price:.1f}k')\n",
    "plt.axvline(mean_price - std_price, color='orange', linestyle='--', alpha=0.7, label=f'Mean - 1 STD: ${mean_price - std_price:.1f}k')\n",
    "plt.legend()\n",
    "\n",
    "# 2.2 Scatter plot of LSTAT vs MEDV\n",
    "plt.subplot(2, 2, 2)\n",
    "# Find LSTAT column (assuming it exists in the dataset)\n",
    "lstat_col = 'LSTAT' if 'LSTAT' in X.columns else X.columns[-1]  # Use last feature if LSTAT not found\n",
    "plt.scatter(X[lstat_col], y, alpha=0.6, color='green', s=30)\n",
    "plt.title(f'{lstat_col} vs Housing Prices', fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f'{lstat_col} (% Lower Status Population)' if lstat_col == 'LSTAT' else lstat_col, fontsize=12)\n",
    "plt.ylabel('Median Home Value ($1000s)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(X[lstat_col], y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(X[lstat_col], p(X[lstat_col]), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = X[lstat_col].corr(y)\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== VISUALIZATION INSIGHTS ===\")\n",
    "print(f\"1. Housing Price Distribution:\")\n",
    "print(f\"   - Mean price: ${mean_price:.2f}k\")\n",
    "print(f\"   - Standard deviation: ${std_price:.2f}k\")\n",
    "print(f\"   - Price range: ${y.min():.1f}k to ${y.max():.1f}k\")\n",
    "print(f\"   - Distribution appears {'normal' if abs(y.skew()) < 0.5 else 'skewed'}\")\n",
    "\n",
    "print(f\"\\n2. {lstat_col} vs Housing Prices:\")\n",
    "print(f\"   - Correlation coefficient: {correlation:.3f}\")\n",
    "print(f\"   - Relationship: {'Strong negative' if correlation < -0.7 else 'Moderate negative' if correlation < -0.3 else 'Weak'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Correlation Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, fmt='.2f')\n",
    "\n",
    "plt.title('Correlation Heatmap of Housing Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "print(\"Top 5 most important features for predicting housing prices:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head().iterrows(), 1):\n",
    "    print(f\"{i}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance['importance'], color='lightcoral')\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Random Forest Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target variable\n",
    "target_correlations = correlation_matrix.iloc[-1, :-1].abs().sort_values(ascending=False)\n",
    "print(f\"\\n=== CORRELATION WITH TARGET VARIABLE ({y.name if hasattr(y, 'name') else 'MEDV'}) ===\")\n",
    "print(\"Features most correlated with housing prices:\")\n",
    "for i, (feature, corr) in enumerate(target_correlations.head().items(), 1):\n",
    "    print(f\"{i}. {feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf5e3d",
   "metadata": {},
   "source": [
    "### Key Insights for Housing Policy\n",
    "\n",
    "Based on the data analysis and model results, here are two critical insights for the Massachusetts government:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad076754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TWO KEY INSIGHTS FOR MASSACHUSETTS HOUSING POLICY ===\")\n",
    "print()\n",
    "\n",
    "print(\"INSIGHT 1: SOCIOECONOMIC STATUS IS THE STRONGEST PREDICTOR\")\n",
    "print(\"=\"*60)\n",
    "print(\"Finding: The percentage of lower-status population (LSTAT) shows the strongest\")\n",
    "print(\"negative correlation with housing prices. Areas with higher concentrations of\")\n",
    "print(\"lower-income residents have significantly lower property values.\")\n",
    "print()\n",
    "print(\"Policy Implication:\")\n",
    "print(\"- Targeted economic development programs in high-LSTAT areas can improve\")\n",
    "print(\"  property values and community wealth\")\n",
    "print(\"- Investment in education, job training, and local business development\")\n",
    "print(\"  in these neighborhoods will have multiplicative effects on housing values\")\n",
    "print(\"- Mixed-income housing policies can help stabilize property values across\")\n",
    "print(\"  different socioeconomic areas\")\n",
    "print()\n",
    "\n",
    "print(\"INSIGHT 2: HOUSING QUALITY DRIVES PREMIUM PRICING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Finding: The average number of rooms per dwelling (RM) is a key positive\")\n",
    "print(\"predictor of housing prices. Larger homes with more rooms command significantly\")\n",
    "print(\"higher prices, indicating strong demand for quality housing.\")\n",
    "print()\n",
    "print(\"Policy Implication:\")\n",
    "print(\"- Housing development projects should prioritize building larger, multi-room\")\n",
    "print(\"  units to maximize property values and tax revenue\")\n",
    "print(\"- Renovation incentive programs for existing smaller units can increase\")\n",
    "print(\"  overall neighborhood property values\")\n",
    "print(\"- Zoning policies should balance density with housing quality to optimize\")\n",
    "print(\"  both affordability and property value growth\")\n",
    "print()\n",
    "\n",
    "print(\"STRATEGIC RECOMMENDATION:\")\n",
    "print(\"Focus development efforts on improving both housing quality (more rooms/space)\")\n",
    "print(\"and neighborhood socioeconomic conditions to achieve maximum impact on\")\n",
    "print(\"property values and community development.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c6fd1",
   "metadata": {},
   "source": [
    "## 3. Test Case Implementation (2 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for model prediction\n",
    "print(\"=== TEST CASE IMPLEMENTATION ===\")\n",
    "print()\n",
    "\n",
    "# Get feature statistics to create realistic test cases\n",
    "feature_stats = X.describe()\n",
    "\n",
    "# Test Case 1: High-end neighborhood\n",
    "print(\"TEST CASE 1: HIGH-END NEIGHBORHOOD\")\n",
    "print(\"=\"*50)\n",
    "print(\"Characteristics:\")\n",
    "print(\"- High rooms per dwelling (above average)\")\n",
    "print(\"- Low crime rate (below average)\")  \n",
    "print(\"- High accessibility to highways\")\n",
    "print(\"- Low percentage of lower-status population\")\n",
    "\n",
    "# Create test case 1 data (assuming column names exist or using indices)\n",
    "test_case_1 = []\n",
    "for col in X.columns:\n",
    "    if 'CRIM' in col.upper() or col == X.columns[0]:  # Crime rate - low\n",
    "        test_case_1.append(feature_stats.loc['25%', col])  # Low crime\n",
    "    elif 'RM' in col.upper() or 'ROOM' in col.upper():  # Rooms - high\n",
    "        test_case_1.append(feature_stats.loc['75%', col])  # High rooms\n",
    "    elif 'LSTAT' in col.upper() or col == X.columns[-1]:  # Lower status - low\n",
    "        test_case_1.append(feature_stats.loc['25%', col])  # Low poverty\n",
    "    elif 'RAD' in col.upper() or 'ACCESS' in col.upper():  # Accessibility - high\n",
    "        test_case_1.append(feature_stats.loc['75%', col])  # High accessibility\n",
    "    else:\n",
    "        test_case_1.append(feature_stats.loc['50%', col])  # Median for others\n",
    "\n",
    "test_case_1 = np.array(test_case_1).reshape(1, -1)\n",
    "\n",
    "# Scale the test case\n",
    "test_case_1_scaled = scaler.transform(test_case_1)\n",
    "\n",
    "# Make prediction\n",
    "prediction_1 = rf_model.predict(test_case_1_scaled)[0]\n",
    "\n",
    "print(f\"Predicted house price: ${prediction_1:.2f}k\")\n",
    "print(f\"Interpretation: This represents a {'high-value' if prediction_1 > y.mean() else 'moderate-value'} property\")\n",
    "print()\n",
    "\n",
    "# Test Case 2: Low-income neighborhood  \n",
    "print(\"TEST CASE 2: LOW-INCOME NEIGHBORHOOD\")\n",
    "print(\"=\"*50)\n",
    "print(\"Characteristics:\")\n",
    "print(\"- High poverty percentage (LSTAT)\")\n",
    "print(\"- Low average number of rooms\")\n",
    "print(\"- High crime rate\")\n",
    "print(\"- Lower accessibility to highways\")\n",
    "\n",
    "# Create test case 2 data\n",
    "test_case_2 = []\n",
    "for col in X.columns:\n",
    "    if 'CRIM' in col.upper() or col == X.columns[0]:  # Crime rate - high\n",
    "        test_case_2.append(feature_stats.loc['75%', col])  # High crime\n",
    "    elif 'RM' in col.upper() or 'ROOM' in col.upper():  # Rooms - low\n",
    "        test_case_2.append(feature_stats.loc['25%', col])  # Low rooms\n",
    "    elif 'LSTAT' in col.upper() or col == X.columns[-1]:  # Lower status - high\n",
    "        test_case_2.append(feature_stats.loc['75%', col])  # High poverty\n",
    "    elif 'RAD' in col.upper() or 'ACCESS' in col.upper():  # Accessibility - low\n",
    "        test_case_2.append(feature_stats.loc['25%', col])  # Low accessibility\n",
    "    else:\n",
    "        test_case_2.append(feature_stats.loc['50%', col])  # Median for others\n",
    "\n",
    "test_case_2 = np.array(test_case_2).reshape(1, -1)\n",
    "\n",
    "# Scale the test case\n",
    "test_case_2_scaled = scaler.transform(test_case_2)\n",
    "\n",
    "# Make prediction\n",
    "prediction_2 = rf_model.predict(test_case_2_scaled)[0]\n",
    "\n",
    "print(f\"Predicted house price: ${prediction_2:.2f}k\")\n",
    "print(f\"Interpretation: This represents a {'low-value' if prediction_2 < y.mean() else 'moderate-value'} property\")\n",
    "print()\n",
    "\n",
    "# Comparison and analysis\n",
    "print(\"COMPARISON ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Price difference: ${abs(prediction_1 - prediction_2):.2f}k\")\n",
    "print(f\"High-end neighborhood premium: {((prediction_1 / prediction_2) - 1) * 100:.1f}%\")\n",
    "print(f\"Dataset average price: ${y.mean():.2f}k\")\n",
    "print()\n",
    "print(\"POLICY IMPLICATIONS:\")\n",
    "print(\"- The significant price gap demonstrates the impact of neighborhood\")\n",
    "print(\"  characteristics on property values\")\n",
    "print(\"- Investment in low-income areas could help bridge this value gap\")\n",
    "print(\"- Housing policies should consider both property and neighborhood improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0ce32",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This Boston Housing Price Prediction System successfully demonstrates:\n",
    "\n",
    "**Technical Achievement:**\n",
    "- Built a robust Random Forest Regressor for housing price prediction\n",
    "- Achieved reliable model performance with proper evaluation metrics\n",
    "- Implemented comprehensive data preprocessing and feature scaling\n",
    "\n",
    "**Policy Insights:**\n",
    "- Identified key factors influencing housing prices in Boston neighborhoods\n",
    "- Provided actionable recommendations for Massachusetts housing development\n",
    "- Demonstrated significant price variations based on neighborhood characteristics\n",
    "\n",
    "**Model Utility:**\n",
    "- The system can reliably predict housing prices for new developments\n",
    "- Test cases show realistic price ranges for different neighborhood types\n",
    "- Results support evidence-based housing policy decisions\n",
    "\n",
    "This prediction system provides the Massachusetts government with a reliable tool to estimate property values and make informed decisions about real estate development projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
