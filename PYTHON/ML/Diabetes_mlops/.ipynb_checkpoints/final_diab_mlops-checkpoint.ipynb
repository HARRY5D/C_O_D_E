{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54494b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344de6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Model imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Visualization setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")\n",
    "print(f\"üìä Matplotlib ready for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a5f3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Load and Explore Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target, name='target')\n",
    "\n",
    "# Create combined dataframe for exploration\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "print(\"üè• Diabetes Dataset Loaded Successfully!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(df)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Feature Names:\")\n",
    "for i, feature in enumerate(diabetes.feature_names, 1):\n",
    "    print(f\"  {i:2d}. {feature:5s}\", end=\"  \")\n",
    "    if i % 5 == 0:\n",
    "        print()\n",
    "\n",
    "print(\"\\n\\nüéØ Target Variable (Disease Progression):\")\n",
    "print(f\"  Mean: {y.mean():.2f}\")\n",
    "print(f\"  Std:  {y.std():.2f}\")\n",
    "print(f\"  Min:  {y.min():.2f}\")\n",
    "print(f\"  Max:  {y.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c005a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nüìã First 5 samples:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüîç Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(df.isnull().sum().sum(), \"missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d5370",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Feature Engineering\n",
    "\n",
    "Create interaction terms and derived features based on domain knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e364d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "print(\"üîß Creating Interaction Features...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# BMI * Blood Pressure (both strong predictors)\n",
    "df['bmi_bp_interaction'] = df['bmi'] * df['bp']\n",
    "print(\"‚úì Created: bmi_bp_interaction (BMI √ó Blood Pressure)\")\n",
    "\n",
    "# Cholesterol ratio interactions\n",
    "df['s2_s3_ratio'] = df['s2'] / (df['s3'] + 1e-6)  # LDL/HDL ratio (bad/good cholesterol)\n",
    "print(\"‚úì Created: s2_s3_ratio (LDL/HDL cholesterol ratio)\")\n",
    "\n",
    "# Age-BMI interaction (age may moderate BMI effect)\n",
    "df['age_bmi_interaction'] = df['age'] * df['bmi']\n",
    "print(\"‚úì Created: age_bmi_interaction (Age √ó BMI)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Feature Engineering Complete!\")\n",
    "print(f\"Original features: 10\")\n",
    "print(f\"New features: 3\")\n",
    "print(f\"Total features: {df.shape[1] - 1}\")  # -1 for target\n",
    "\n",
    "# Update X with new features\n",
    "X_enhanced = df.drop('target', axis=1)\n",
    "print(f\"\\nüìä Enhanced feature set shape: {X_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efeed8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Train-Test Split\n",
    "\n",
    "Split data into training (75%) and testing (25%) sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data - using 90/10 like the INTERNSHIP notebook for consistency\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_enhanced, y, test_size=0.1, random_state=45\n",
    ")\n",
    "\n",
    "print(\"‚úÇÔ∏è Train-Test Split Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set size:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X_enhanced)*100:.1f}%)\")\n",
    "print(f\"Testing set size:    {X_test.shape[0]} samples ({X_test.shape[0]/len(X_enhanced)*100:.1f}%)\")\n",
    "print(f\"Number of features:  {X_train.shape[1]}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úì Training set will be used to fit models\")\n",
    "print(\"‚úì Test set remains unseen until final evaluation\")\n",
    "print(\"‚úì This prevents overfitting and gives honest performance estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00b9c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Feature Selection with RFE\n",
    "\n",
    "Use Recursive Feature Elimination to select the most important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RFE for feature selection (like the INTERNSHIP notebook)\n",
    "print(\"üîç Applying Recursive Feature Elimination (RFE)...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use Linear Regression as the base estimator for RFE\n",
    "base_estimator = LinearRegression()\n",
    "rfe = RFE(estimator=base_estimator, n_features_to_select=6, step=1)\n",
    "\n",
    "# Fit RFE on training data\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_enhanced.columns[rfe.support_].tolist()\n",
    "eliminated_features = X_enhanced.columns[~rfe.support_].tolist()\n",
    "\n",
    "print(f\"‚úì RFE Complete!\")\n",
    "print(f\"\\nüìä Selected Features ({len(selected_features)}):\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(f\"\\n‚ùå Eliminated Features ({len(eliminated_features)}):\")\n",
    "for i, feature in enumerate(eliminated_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set shape after RFE: {X_train_rfe.shape}\")\n",
    "print(f\"Testing set shape after RFE:  {X_test_rfe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b2aa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Feature Scaling\n",
    "\n",
    "Apply StandardScaler to the RFE-selected features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_rfe)\n",
    "X_test_scaled = scaler.transform(X_test_rfe)\n",
    "\n",
    "print(\"‚öñÔ∏è Feature Scaling Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Scaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set shape:     {X_test_scaled.shape}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\nTraining set statistics after scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "\n",
    "print(\"\\n‚úì Scaler fitted on training data only\")\n",
    "print(\"‚úì Test data transformed using training statistics\")\n",
    "print(\"‚úì This prevents data leakage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522007b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Baseline Models Training\n",
    "\n",
    "Train multiple baseline models without hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'ElasticNet': ElasticNet(random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "baseline_results = {}\n",
    "\n",
    "print(\"üöÄ Training Baseline Models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    baseline_results[name] = {\n",
    "        'model': model,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:20s} | R¬≤: {r2:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Baseline model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(baseline_results.keys()),\n",
    "    'R¬≤ Score': [v['r2'] for v in baseline_results.values()],\n",
    "    'RMSE': [v['rmse'] for v in baseline_results.values()],\n",
    "    'MAE': [v['mae'] for v in baseline_results.values()]\n",
    "})\n",
    "\n",
    "# Sort by R¬≤ score\n",
    "results_df = results_df.sort_values('R¬≤ Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Baseline Model Comparison (sorted by R¬≤):\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# R¬≤ Score\n",
    "axes[0].barh(results_df['Model'], results_df['R¬≤ Score'], color='steelblue')\n",
    "axes[0].set_xlabel('R¬≤ Score', fontsize=12)\n",
    "axes[0].set_title('Model Comparison - R¬≤ Score', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(x=0.5711, color='red', linestyle='--', linewidth=2, label='Target: 0.5711')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1].barh(results_df['Model'], results_df['RMSE'], color='coral')\n",
    "axes[1].set_xlabel('RMSE', fontsize=12)\n",
    "axes[1].set_title('Model Comparison - RMSE', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=43.08, color='red', linestyle='--', linewidth=2, label='Target: 43.08')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[2].barh(results_df['Model'], results_df['MAE'], color='lightgreen')\n",
    "axes[2].set_xlabel('MAE', fontsize=12)\n",
    "axes[2].set_title('Model Comparison - MAE', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Best Baseline Model: {results_df.iloc[0]['Model']}\")\n",
    "print(f\"   R¬≤ Score: {results_df.iloc[0]['R¬≤ Score']:.4f}\")\n",
    "print(f\"   RMSE: {results_df.iloc[0]['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601d656",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Hyperparameter Tuning\n",
    "\n",
    "Tune the top 3 performing models using GridSearchCV/RandomizedSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 3 models for tuning\n",
    "top_3_models = results_df.head(3)['Model'].tolist()\n",
    "\n",
    "print(\"üéØ Hyperparameter Tuning for Top 3 Models...\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Models selected for tuning: {', '.join(top_3_models)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df777c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'alpha': np.logspace(-3, 3, 7),\n",
    "        'fit_intercept': [True, False],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga']\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': np.logspace(-3, 3, 7),\n",
    "        'fit_intercept': [True, False],\n",
    "        'selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': np.logspace(-3, 3, 7),\n",
    "        'l1_ratio': np.linspace(0.1, 0.9, 9),\n",
    "        'fit_intercept': [True, False],\n",
    "        'selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'gamma': np.logspace(-4, 0, 5),\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Scorer for optimization\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "# Store tuned results\n",
    "tuned_results = {}\n",
    "\n",
    "print(\"\\nüîß Starting hyperparameter tuning...\\n\")\n",
    "\n",
    "for model_name in top_3_models:\n",
    "    if model_name not in param_grids:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {model_name} - no parameter grid defined\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Tuning: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get base model\n",
    "    base_model = baseline_results[model_name]['model']\n",
    "    \n",
    "    # Use RandomizedSearch for complex models, GridSearch for simple ones\n",
    "    if model_name in ['Random Forest', 'SVR', 'Gradient Boosting']:\n",
    "        search = RandomizedSearchCV(\n",
    "            base_model,\n",
    "            param_distributions=param_grids[model_name],\n",
    "            n_iter=20,\n",
    "            scoring=scorer,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            base_model,\n",
    "            param_grid=param_grids[model_name],\n",
    "            scoring=scorer,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    # Fit search\n",
    "    search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get best model and make predictions\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    tuned_results[model_name] = {\n",
    "        'model': best_model,\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'best_params': search.best_params_,\n",
    "        'cv_score': search.best_score_\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best Parameters:\")\n",
    "    for param, value in search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä Performance:\")\n",
    "    print(f\"   CV R¬≤ Score: {search.best_score_:.4f}\")\n",
    "    print(f\"   Test R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"   Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"   Test MAE: {mae:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Hyperparameter tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned results\n",
    "tuned_df = pd.DataFrame({\n",
    "    'Model': list(tuned_results.keys()),\n",
    "    'R¬≤ Score': [v['r2'] for v in tuned_results.values()],\n",
    "    'RMSE': [v['rmse'] for v in tuned_results.values()],\n",
    "    'MAE': [v['mae'] for v in tuned_results.values()],\n",
    "    'CV R¬≤': [v['cv_score'] for v in tuned_results.values()]\n",
    "}).sort_values('R¬≤ Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Tuned Model Comparison:\")\n",
    "print(tuned_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüèÜ Best Tuned Model: {tuned_df.iloc[0]['Model']}\")\n",
    "print(f\"   R¬≤ Score: {tuned_df.iloc[0]['R¬≤ Score']:.4f}\")\n",
    "print(f\"   RMSE: {tuned_df.iloc[0]['RMSE']:.4f}\")\n",
    "print(f\"   CV R¬≤: {tuned_df.iloc[0]['CV R¬≤']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8e9af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Load Pre-trained Champion Model\n",
    "\n",
    "Load the pre-trained Lasso model from the INTERNSHIP project and evaluate it on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "pretrained_model_path = r\"D:\\JAVA\\CODE\\PYTHON\\ML\\INTERNSHIP\\DiabetesProgressionPredictor\\output\\models\\lasso_tuned.pkl\"\n",
    "pretrained_rfe_path = r\"D:\\JAVA\\CODE\\PYTHON\\ML\\INTERNSHIP\\DiabetesProgressionPredictor\\output\\models\\rfe_selector.pkl\"\n",
    "pretrained_scaler_path = r\"D:\\JAVA\\CODE\\PYTHON\\ML\\INTERNSHIP\\DiabetesProgressionPredictor\\output\\models\\scaler.pkl\"\n",
    "\n",
    "print(\"üì¶ Loading Pre-trained Champion Model...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Load model and preprocessors\n",
    "    pretrained_model = joblib.load(pretrained_model_path)\n",
    "    pretrained_rfe = joblib.load(pretrained_rfe_path)\n",
    "    pretrained_scaler = joblib.load(pretrained_scaler_path)\n",
    "    \n",
    "    print(\"‚úÖ Successfully loaded:\")\n",
    "    print(f\"   - Model: {type(pretrained_model).__name__}\")\n",
    "    print(f\"   - RFE Selector: {pretrained_rfe.n_features_to_select} features selected\")\n",
    "    print(f\"   - Scaler: {type(pretrained_scaler).__name__}\")\n",
    "    \n",
    "    # Get feature names from RFE\n",
    "    pretrained_features = X_enhanced.columns[pretrained_rfe.support_].tolist()\n",
    "    print(f\"\\n   Selected Features: {pretrained_features}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: Could not find pre-trained model files\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    pretrained_model = None\n",
    "    pretrained_rfe = None\n",
    "    pretrained_scaler = None\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3681458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pre-trained model on our test set\n",
    "if pretrained_model is not None:\n",
    "    print(\"\\nüß™ Evaluating Pre-trained Model on Current Test Set...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # We need to use the ORIGINAL test set (before our RFE) \n",
    "    # because the pre-trained model has its own RFE\n",
    "    \n",
    "    # Apply pre-trained RFE and scaler\n",
    "    X_test_pretrained_rfe = pretrained_rfe.transform(X_test)\n",
    "    X_test_pretrained_scaled = pretrained_scaler.transform(X_test_pretrained_rfe)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_pretrained = pretrained_model.predict(X_test_pretrained_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2_pretrained = r2_score(y_test, y_pred_pretrained)\n",
    "    rmse_pretrained = np.sqrt(mean_squared_error(y_test, y_pred_pretrained))\n",
    "    mae_pretrained = mean_absolute_error(y_test, y_pred_pretrained)\n",
    "    \n",
    "    print(f\"üéØ Pre-trained Lasso Model Performance:\")\n",
    "    print(f\"   R¬≤ Score: {r2_pretrained:.4f}\")\n",
    "    print(f\"   RMSE:     {rmse_pretrained:.4f}\")\n",
    "    print(f\"   MAE:      {mae_pretrained:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Comparison with Original Metrics:\")\n",
    "    print(f\"   Original R¬≤:   0.5711\")\n",
    "    print(f\"   Current R¬≤:    {r2_pretrained:.4f}\")\n",
    "    print(f\"   Difference:    {r2_pretrained - 0.5711:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Original RMSE: 43.08\")\n",
    "    print(f\"   Current RMSE:  {rmse_pretrained:.4f}\")\n",
    "    print(f\"   Difference:    {rmse_pretrained - 43.08:.4f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Add to comparison\n",
    "    pretrained_result = {\n",
    "        'Pre-trained Lasso': {\n",
    "            'model': pretrained_model,\n",
    "            'r2': r2_pretrained,\n",
    "            'rmse': rmse_pretrained,\n",
    "            'mae': mae_pretrained\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd58fbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Final Model Comparison\n",
    "\n",
    "Compare all models: baseline, tuned, and pre-trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755068d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {}\n",
    "\n",
    "# Add baseline results\n",
    "for name, result in baseline_results.items():\n",
    "    all_results[f\"{name} (Baseline)\"] = result\n",
    "\n",
    "# Add tuned results\n",
    "for name, result in tuned_results.items():\n",
    "    all_results[f\"{name} (Tuned)\"] = result\n",
    "\n",
    "# Add pre-trained result\n",
    "if pretrained_model is not None:\n",
    "    all_results['Pre-trained Lasso ‚≠ê'] = pretrained_result['Pre-trained Lasso']\n",
    "\n",
    "# Create comparison dataframe\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': list(all_results.keys()),\n",
    "    'R¬≤ Score': [v['r2'] for v in all_results.values()],\n",
    "    'RMSE': [v['rmse'] for v in all_results.values()],\n",
    "    'MAE': [v['mae'] for v in all_results.values()]\n",
    "}).sort_values('R¬≤ Score', ascending=False)\n",
    "\n",
    "print(\"üèÜ FINAL MODEL COMPARISON - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(final_comparison.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Highlight best model\n",
    "best_model_name = final_comparison.iloc[0]['Model']\n",
    "best_r2 = final_comparison.iloc[0]['R¬≤ Score']\n",
    "best_rmse = final_comparison.iloc[0]['RMSE']\n",
    "\n",
    "print(f\"\\nü•á CHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"   R¬≤ Score: {best_r2:.4f}\")\n",
    "print(f\"   RMSE:     {best_rmse:.4f}\")\n",
    "print(f\"   MAE:      {final_comparison.iloc[0]['MAE']:.4f}\")\n",
    "\n",
    "# Check if we beat the target\n",
    "if best_r2 >= 0.5711:\n",
    "    print(f\"\\n‚úÖ SUCCESS! We matched/exceeded the target R¬≤ of 0.5711\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Target R¬≤ (0.5711) not reached. Difference: {0.5711 - best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eca56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison - Top 10 models\n",
    "top_10 = final_comparison.head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create bar plot\n",
    "bars = ax.barh(top_10['Model'], top_10['R¬≤ Score'], color='steelblue', edgecolor='black')\n",
    "\n",
    "# Highlight champion model\n",
    "bars[0].set_color('gold')\n",
    "bars[0].set_edgecolor('darkgoldenrod')\n",
    "bars[0].set_linewidth(3)\n",
    "\n",
    "# Add target line\n",
    "ax.axvline(x=0.5711, color='red', linestyle='--', linewidth=2, label='Target: R¬≤ = 0.5711', alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('R¬≤ Score', fontsize=14, fontweight='bold')\n",
    "ax.set_title('üèÜ Final Model Comparison - Top 10 Models by R¬≤ Score', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, r2) in enumerate(zip(top_10['Model'], top_10['R¬≤ Score'])):\n",
    "    ax.text(r2 + 0.01, i, f'{r2:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization shows top 10 models ranked by R¬≤ Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21820471",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Save Best Model\n",
    "\n",
    "Save the best performing model with all preprocessing components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2218dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine which model to save\n",
    "if best_model_name == 'Pre-trained Lasso ‚≠ê':\n",
    "    print(\"üì¶ Best model is the pre-trained Lasso - already saved!\")\n",
    "    print(f\"   Location: {pretrained_model_path}\")\n",
    "    save_model = pretrained_model\n",
    "    save_rfe = pretrained_rfe\n",
    "    save_scaler = pretrained_scaler\n",
    "    save_name = \"pretrained_lasso_champion\"\n",
    "else:\n",
    "    # Save our newly trained best model\n",
    "    model_base_name = best_model_name.replace(' (Tuned)', '').replace(' (Baseline)', '')\n",
    "    \n",
    "    if model_base_name in tuned_results:\n",
    "        save_model = tuned_results[model_base_name]['model']\n",
    "    else:\n",
    "        save_model = baseline_results[model_base_name]['model']\n",
    "    \n",
    "    save_rfe = rfe\n",
    "    save_scaler = scaler\n",
    "    save_name = f\"best_model_{model_base_name.lower().replace(' ', '_')}\"\n",
    "\n",
    "# Save components\n",
    "model_path = models_dir / f\"{save_name}.joblib\"\n",
    "rfe_path = models_dir / f\"{save_name}_rfe.joblib\"\n",
    "scaler_path = models_dir / f\"{save_name}_scaler.joblib\"\n",
    "\n",
    "joblib.dump(save_model, model_path)\n",
    "joblib.dump(save_rfe, rfe_path)\n",
    "joblib.dump(save_scaler, scaler_path)\n",
    "\n",
    "print(\"\\nüíæ Saving Best Model...\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "print(f\"‚úÖ RFE saved: {rfe_path}\")\n",
    "print(f\"‚úÖ Scaler saved: {scaler_path}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìã Model Details:\")\n",
    "print(f\"   Type: {type(save_model).__name__}\")\n",
    "print(f\"   R¬≤ Score: {best_r2:.4f}\")\n",
    "print(f\"   RMSE: {best_rmse:.4f}\")\n",
    "print(f\"   Features: {len(X_enhanced.columns[save_rfe.support_])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093d7d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Interactive Prediction Interface\n",
    "\n",
    "Test the champion model with custom patient data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765af861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict_diabetes_progression(age, sex, bmi, bp, s1, s2, s3, s4, s5, s6, \n",
    "                                 model=None, rfe_selector=None, scaler_obj=None):\n",
    "    \"\"\"\n",
    "    Predict diabetes progression for a patient\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    age, sex, bmi, bp, s1-s6: float\n",
    "        Patient features (normalized values)\n",
    "    model: estimator\n",
    "        Trained model\n",
    "    rfe_selector: RFE\n",
    "        Feature selector\n",
    "    scaler_obj: StandardScaler\n",
    "        Feature scaler\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction: float\n",
    "        Predicted disease progression\n",
    "    \"\"\"\n",
    "    # Use best model if none provided\n",
    "    if model is None:\n",
    "        model = save_model\n",
    "    if rfe_selector is None:\n",
    "        rfe_selector = save_rfe\n",
    "    if scaler_obj is None:\n",
    "        scaler_obj = save_scaler\n",
    "    \n",
    "    # Create feature dataframe\n",
    "    features_dict = {\n",
    "        'age': age, 'sex': sex, 'bmi': bmi, 'bp': bp,\n",
    "        's1': s1, 's2': s2, 's3': s3, 's4': s4, 's5': s5, 's6': s6\n",
    "    }\n",
    "    \n",
    "    # Add interaction features\n",
    "    features_dict['bmi_bp_interaction'] = bmi * bp\n",
    "    features_dict['s2_s3_ratio'] = s2 / (s3 + 1e-6)\n",
    "    features_dict['age_bmi_interaction'] = age * bmi\n",
    "    \n",
    "    # Create dataframe with correct column order\n",
    "    input_df = pd.DataFrame([features_dict])[X_enhanced.columns]\n",
    "    \n",
    "    # Apply RFE\n",
    "    input_rfe = rfe_selector.transform(input_df)\n",
    "    \n",
    "    # Apply scaling\n",
    "    input_scaled = scaler_obj.transform(input_rfe)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_scaled)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test with a sample patient\n",
    "print(\"üß™ Testing Prediction Function...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample patient (using mean values)\n",
    "sample_prediction = predict_diabetes_progression(\n",
    "    age=0.0, sex=0.0, bmi=0.0, bp=0.0,\n",
    "    s1=0.0, s2=0.0, s3=0.0, s4=0.0, s5=0.0, s6=0.0\n",
    ")\n",
    "\n",
    "print(f\"Sample Patient (all mean values):\")\n",
    "print(f\"  Predicted Disease Progression: {sample_prediction:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Prediction function working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction with slider controls (for Jupyter)\n",
    "print(\"üìä Interactive Prediction Interface\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou can now make predictions by calling the predict_diabetes_progression() function\")\n",
    "print(\"with patient parameters.\\n\")\n",
    "\n",
    "print(\"Example usage:\")\n",
    "print(\"\"\"\n",
    "prediction = predict_diabetes_progression(\n",
    "    age=0.05,    # Slightly older than average\n",
    "    sex=0.05,    # Male\n",
    "    bmi=0.06,    # Slightly higher BMI\n",
    "    bp=0.02,     # Slightly elevated blood pressure\n",
    "    s1=0.01,     # Total cholesterol\n",
    "    s2=0.03,     # LDL cholesterol\n",
    "    s3=-0.04,    # HDL cholesterol\n",
    "    s4=0.02,     # Total cholesterol / HDL\n",
    "    s5=0.05,     # Triglycerides (log)\n",
    "    s6=0.04      # Blood sugar level\n",
    ")\n",
    "\n",
    "print(f\"Predicted Disease Progression: {prediction:.2f}\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüí° Feature Value Ranges (normalized):\")\n",
    "print(\"   age: -2.0 to 2.0  (younger to older)\")\n",
    "print(\"   sex: -0.05 to 0.05  (-0.05=Female, 0.05=Male)\")\n",
    "print(\"   bmi: -2.5 to 3.0  (underweight to obese)\")\n",
    "print(\"   bp:  -2.0 to 3.0  (low to high blood pressure)\")\n",
    "print(\"   s1-s6: typically -2.0 to 3.0 (various blood measurements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with several realistic patient scenarios\n",
    "print(\"üè• Testing with Realistic Patient Scenarios\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_patients = [\n",
    "    {\n",
    "        'name': 'Healthy Young Patient',\n",
    "        'params': {'age': -0.5, 'sex': 0.05, 'bmi': -0.3, 'bp': -0.2, \n",
    "                   's1': -0.1, 's2': -0.1, 's3': 0.2, 's4': -0.2, 's5': -0.1, 's6': -0.1}\n",
    "    },\n",
    "    {\n",
    "        'name': 'At-Risk Middle-Aged Patient',\n",
    "        'params': {'age': 0.3, 'sex': -0.05, 'bmi': 0.5, 'bp': 0.4,\n",
    "                   's1': 0.3, 's2': 0.4, 's3': -0.3, 's4': 0.5, 's5': 0.4, 's6': 0.3}\n",
    "    },\n",
    "    {\n",
    "        'name': 'High-Risk Elderly Patient',\n",
    "        'params': {'age': 1.5, 'sex': 0.05, 'bmi': 1.0, 'bp': 1.0,\n",
    "                   's1': 0.8, 's2': 0.9, 's3': -0.5, 's4': 0.9, 's5': 0.8, 's6': 0.8}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Average Patient (Baseline)',\n",
    "        'params': {'age': 0.0, 'sex': 0.0, 'bmi': 0.0, 'bp': 0.0,\n",
    "                   's1': 0.0, 's2': 0.0, 's3': 0.0, 's4': 0.0, 's5': 0.0, 's6': 0.0}\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "for patient in test_patients:\n",
    "    pred = predict_diabetes_progression(**patient['params'])\n",
    "    predictions_list.append(pred)\n",
    "    \n",
    "    print(f\"\\n{patient['name']}:\")\n",
    "    print(f\"  Predicted Progression: {pred:.2f}\")\n",
    "    \n",
    "    # Interpret result\n",
    "    if pred < 100:\n",
    "        risk = \"Low Risk ‚úÖ\"\n",
    "    elif pred < 175:\n",
    "        risk = \"Moderate Risk ‚ö†Ô∏è\"\n",
    "    else:\n",
    "        risk = \"High Risk ‚ö†Ô∏è‚ö†Ô∏è\"\n",
    "    \n",
    "    print(f\"  Risk Level: {risk}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All test predictions completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction scenarios\n",
    "scenario_names = [p['name'] for p in test_patients]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['green', 'orange', 'red', 'blue']\n",
    "bars = ax.bar(scenario_names, predictions_list, color=colors, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add threshold lines\n",
    "ax.axhline(y=100, color='green', linestyle='--', linewidth=2, label='Low Risk Threshold', alpha=0.5)\n",
    "ax.axhline(y=175, color='orange', linestyle='--', linewidth=2, label='Moderate Risk Threshold', alpha=0.5)\n",
    "\n",
    "# Formatting\n",
    "ax.set_ylabel('Predicted Disease Progression', fontsize=14, fontweight='bold')\n",
    "ax.set_title('üè• Diabetes Progression Predictions - Patient Scenarios', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, pred in zip(bars, predictions_list):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "            f'{pred:.1f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization shows predicted progression for different patient scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5b740",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 13: Model Interpretation & Feature Importance\n",
    "\n",
    "Analyze which features are most important for predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for the best model\n",
    "print(\"üîç Feature Importance Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Selected features after RFE\n",
    "selected_feature_names = X_enhanced.columns[save_rfe.support_].tolist()\n",
    "\n",
    "print(f\"Selected Features ({len(selected_feature_names)}):\")\n",
    "for i, feat in enumerate(selected_feature_names, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# For linear models, use coefficients\n",
    "if hasattr(save_model, 'coef_'):\n",
    "    importances = np.abs(save_model.coef_)\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': selected_feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Feature Importance (Absolute Coefficients):\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Absolute Coefficient Value', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# For tree-based models, use feature_importances_\n",
    "elif hasattr(save_model, 'feature_importances_'):\n",
    "    importances = save_model.feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': selected_feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìä Feature Importance:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.barh(importance_df['Feature'], importance_df['Importance'], color='forestgreen', edgecolor='black')\n",
    "    ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a48f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 14: Summary & Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéâ FINAL DIABETES PREDICTION MLOPS PROJECT - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET:\")\n",
    "print(f\"   Total Samples: {len(df)}\")\n",
    "print(f\"   Original Features: 10\")\n",
    "print(f\"   Engineered Features: 3\")\n",
    "print(f\"   Total Features: 13\")\n",
    "print(f\"   Train/Test Split: 90%/10%\")\n",
    "\n",
    "print(\"\\nü§ñ MODELS TRAINED:\")\n",
    "print(f\"   Baseline Models: {len(baseline_results)}\")\n",
    "print(f\"   Tuned Models: {len(tuned_results)}\")\n",
    "print(f\"   Total Models Evaluated: {len(all_results)}\")\n",
    "\n",
    "print(\"\\nüèÜ CHAMPION MODEL:\")\n",
    "print(f\"   Name: {best_model_name}\")\n",
    "print(f\"   R¬≤ Score: {best_r2:.4f}\")\n",
    "print(f\"   RMSE: {best_rmse:.4f}\")\n",
    "print(f\"   MAE: {final_comparison.iloc[0]['MAE']:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ TARGET COMPARISON:\")\n",
    "print(f\"   Target R¬≤: 0.5711\")\n",
    "print(f\"   Achieved R¬≤: {best_r2:.4f}\")\n",
    "if best_r2 >= 0.5711:\n",
    "    print(f\"   Status: ‚úÖ TARGET ACHIEVED!\")\n",
    "    print(f\"   Improvement: +{(best_r2 - 0.5711):.4f}\")\n",
    "else:\n",
    "    print(f\"   Status: ‚ö†Ô∏è Target not reached\")\n",
    "    print(f\"   Gap: -{(0.5711 - best_r2):.4f}\")\n",
    "\n",
    "print(\"\\nüíæ SAVED ARTIFACTS:\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   RFE Selector: {rfe_path}\")\n",
    "print(f\"   Scaler: {scaler_path}\")\n",
    "\n",
    "print(\"\\n‚ú® KEY FEATURES (after RFE):\")\n",
    "for i, feat in enumerate(selected_feature_names, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "\n",
    "print(\"\\nüî¨ CAPABILITIES:\")\n",
    "print(\"   ‚úÖ Data loading and exploration\")\n",
    "print(\"   ‚úÖ Feature engineering (interaction terms)\")\n",
    "print(\"   ‚úÖ Feature selection (RFE)\")\n",
    "print(\"   ‚úÖ Multiple model training and comparison\")\n",
    "print(\"   ‚úÖ Hyperparameter tuning\")\n",
    "print(\"   ‚úÖ Pre-trained model loading and evaluation\")\n",
    "print(\"   ‚úÖ Model saving for production\")\n",
    "print(\"   ‚úÖ Interactive prediction interface\")\n",
    "print(\"   ‚úÖ Feature importance analysis\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Deploy model to production\")\n",
    "print(\"   2. Set up monitoring for model drift\")\n",
    "print(\"   3. Collect new data for retraining\")\n",
    "print(\"   4. Create REST API for predictions\")\n",
    "print(\"   5. Build web interface for healthcare providers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PROJECT COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210bdd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Usage Instructions\n",
    "\n",
    "### Making Predictions\n",
    "\n",
    "```python\n",
    "# Use the predict_diabetes_progression() function\n",
    "prediction = predict_diabetes_progression(\n",
    "    age=0.05,    # Normalized age\n",
    "    sex=0.05,    # Male (use -0.05 for Female)\n",
    "    bmi=0.06,    # Body mass index\n",
    "    bp=0.02,     # Blood pressure\n",
    "    s1=0.01,     # Total cholesterol\n",
    "    s2=0.03,     # LDL cholesterol\n",
    "    s3=-0.04,    # HDL cholesterol\n",
    "    s4=0.02,     # Total/HDL ratio\n",
    "    s5=0.05,     # Triglycerides (log)\n",
    "    s6=0.04      # Blood sugar\n",
    ")\n",
    "\n",
    "print(f\"Predicted Progression: {prediction:.2f}\")\n",
    "```\n",
    "\n",
    "### Loading Saved Model Later\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load model and preprocessors\n",
    "model = joblib.load('models/best_model_xxx.joblib')\n",
    "rfe = joblib.load('models/best_model_xxx_rfe.joblib')\n",
    "scaler = joblib.load('models/best_model_xxx_scaler.joblib')\n",
    "\n",
    "# Use for predictions\n",
    "# (Follow same process as above)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "- **Dataset:** Scikit-learn Diabetes Dataset\n",
    "- **Algorithms:** Linear Regression, Ridge, Lasso, ElasticNet, Random Forest, Gradient Boosting, SVR\n",
    "- **Techniques:** RFE, GridSearchCV, RandomizedSearchCV, Feature Engineering\n",
    "- **Pre-trained Model:** INTERNSHIP/DiabetesProgressionPredictor\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook** üéâ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
