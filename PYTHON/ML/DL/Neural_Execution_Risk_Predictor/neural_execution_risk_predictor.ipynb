{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c62f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "````xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Neural Execution Risk Predictor\n",
    "## End-to-End Deep Learning System\n",
    "\n",
    "**Project Goal:** Build a Deep Learning system that predicts runtime execution risk of autonomous agents before execution.\n",
    "\n",
    "**Risk Levels:**\n",
    "- **LOW_RISK (0)**: Safe to execute with normal limits\n",
    "- **MEDIUM_RISK (1)**: Execute with moderate restrictions\n",
    "- **HIGH_RISK (2)**: Execute with tight limits or block entirely\n",
    "\n",
    "**Tech Stack:**\n",
    "- TensorFlow 2.x\n",
    "- Structured/tabular features (NO NLP)\n",
    "- FastAPI for serving\n",
    "- Production-ready and explainable\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Process Mining\n",
    "import pm4py\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# GPU Configuration\n",
    "print(\"\\nGPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1. Data Collection & Feature Engineering\n",
    "\n",
    "We use a **hybrid dataset** combining:\n",
    "1. **BPI Challenge 2012** event logs (real execution traces)\n",
    "2. **Synthetic agent execution plans** (with explicit risk features)\n",
    "\n",
    "### Feature Schema (9 input features):\n",
    "- `num_steps`: Number of execution steps\n",
    "- `num_tools`: Number of distinct tools used\n",
    "- `tool_diversity`: Diversity of tools\n",
    "- `has_high_risk_tool`: Boolean flag for risky tools (0/1)\n",
    "- `est_tokens`: Estimated token budget\n",
    "- `max_retries`: Maximum retry attempts\n",
    "- `sequential_tool_calls`: Consecutive repeated tool calls\n",
    "- `plan_depth`: Nesting depth of execution plan\n",
    "- `time_limit_sec`: Time limit for execution\n",
    "\n",
    "### Target:\n",
    "- `failure_label`: 0=LOW_RISK, 1=MEDIUM_RISK, 2=HIGH_RISK\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Define project paths\n",
    "PROJECT_ROOT = r\"D:\\JAVA\\CODE\\PYTHON\\ML\\DL\\Neural Execution Risk Predictor\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "SCRIPTS_DIR = os.path.join(PROJECT_ROOT, \"scripts\")\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, \"model\")\n",
    "REPORTS_DIR = os.path.join(PROJECT_ROOT, \"reports\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [DATA_DIR, MODEL_DIR, REPORTS_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"Project directories ready!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 1.1 Extract Features from BPI Challenge 2012 XES File\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Add scripts directory to path\n",
    "sys.path.append(SCRIPTS_DIR)\n",
    "\n",
    "from extract_bpi_features import extract_bpi_features\n",
    "\n",
    "# Extract BPI features\n",
    "xes_file_path = os.path.join(PROJECT_ROOT, \"new_BPI_Challenge_2012.xes\")\n",
    "bpi_csv_path = os.path.join(DATA_DIR, \"bpi_features.csv\")\n",
    "\n",
    "if not os.path.exists(bpi_csv_path):\n",
    "    print(\"Extracting features from BPI Challenge 2012 XES file...\")\n",
    "    df_bpi = extract_bpi_features(xes_file_path, bpi_csv_path)\n",
    "else:\n",
    "    print(\"Loading existing BPI features...\")\n",
    "    df_bpi = pd.read_csv(bpi_csv_path)\n",
    "\n",
    "print(f\"\\nBPI Dataset Shape: {df_bpi.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_bpi.head())\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 1.2 Generate Synthetic Agent Execution Plans\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "from generate_synthetic_plans import save_synthetic_dataset\n",
    "\n",
    "# Generate synthetic plans\n",
    "synthetic_csv_path = os.path.join(DATA_DIR, \"synthetic_plans.csv\")\n",
    "\n",
    "if not os.path.exists(synthetic_csv_path):\n",
    "    print(\"Generating synthetic execution plans...\")\n",
    "    df_synthetic = save_synthetic_dataset(\n",
    "        synthetic_csv_path, \n",
    "        num_samples=2000, \n",
    "        include_edge_cases=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading existing synthetic plans...\")\n",
    "    df_synthetic = pd.read_csv(synthetic_csv_path)\n",
    "\n",
    "print(f\"\\nSynthetic Dataset Shape: {df_synthetic.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_synthetic.head())\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 1.3 Combine Datasets into Hybrid Dataset\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Remove ID columns for combining\n",
    "df_bpi_features = df_bpi.drop(columns=['case_id'])\n",
    "df_synthetic_features = df_synthetic.drop(columns=['plan_id'])\n",
    "\n",
    "# Combine datasets\n",
    "df_combined = pd.concat([df_bpi_features, df_synthetic_features], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined Dataset Shape: {df_combined.shape}\")\n",
    "print(f\"\\nTotal Samples: {len(df_combined)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RISK LABEL DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "label_counts = df_combined['failure_label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "print(f\"\\nLOW_RISK (0): {label_counts[0]} ({label_counts[0]/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"MEDIUM_RISK (1): {label_counts[1]} ({label_counts[1]/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"HIGH_RISK (2): {label_counts[2]} ({label_counts[2]/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "# Save combined dataset\n",
    "combined_csv_path = os.path.join(DATA_DIR, \"execution_risk_dataset.csv\")\n",
    "df_combined.to_csv(combined_csv_path, index=False)\n",
    "print(f\"\\nSaved combined dataset to: {combined_csv_path}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### 1.4 Exploratory Data Analysis\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Display summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(df_combined.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "print(df_combined.isnull().sum())\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Visualize feature distributions by risk level\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "feature_cols = [col for col in df_combined.columns if col != 'failure_label']\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    for label in [0, 1, 2]:\n",
    "        data = df_combined[df_combined['failure_label'] == label][col]\n",
    "        ax.hist(data, alpha=0.5, label=f'Risk {label}', bins=20)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(REPORTS_DIR, 'feature_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Feature distribution plot saved!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "### Requirements:\n",
    "1. Convert boolean features to 0/1 (already done)\n",
    "2. Normalize numeric features using StandardScaler\n",
    "3. Do NOT scale labels\n",
    "4. Split into train/validation/test sets\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Separate features and labels\n",
    "X = df_combined.drop(columns=['failure_label']).values\n",
    "y = df_combined['failure_label'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Feature names for later use\n",
    "feature_names = [col for col in df_combined.columns if col != 'failure_label']\n",
    "print(f\"\\nFeature names: {feature_names}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 of 0.85 â‰ˆ 0.15\n",
    ")\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"  Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check label distribution in each set\n",
    "print(\"\\nLabel Distribution in Training Set:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(\"\\nLabel Distribution in Validation Set:\")\n",
    "print(pd.Series(y_val).value_counts().sort_index())\n",
    "print(\"\\nLabel Distribution in Test Set:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature Normalization Complete!\")\n",
    "print(f\"  Mean of training features: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"  Std of training features: {X_train_scaled.std(axis=0)}\")\n",
    "\n",
    "# Save scaler for later use in API\n",
    "import joblib\n",
    "scaler_path = os.path.join(MODEL_DIR, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\nScaler saved to: {scaler_path}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Convert labels to categorical (one-hot encoding) for neural network\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, num_classes=3)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "print(\"Label Encoding:\")\n",
    "print(f\"  Original label shape: {y_train.shape}\")\n",
    "print(f\"  Categorical label shape: {y_train_cat.shape}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Original: {y_train[0]}\")\n",
    "print(f\"  Categorical: {y_train_cat[0]}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3. Model Architecture\n",
    "\n",
    "### Locked Architecture:\n",
    "```\n",
    "Input (9 features)\n",
    "  â†“\n",
    "Dense(64, ReLU)\n",
    "  â†“\n",
    "Dropout(0.2)\n",
    "  â†“\n",
    "Dense(32, ReLU)\n",
    "  â†“\n",
    "Dense(3, Softmax)\n",
    "```\n",
    "\n",
    "### Training Configuration:\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Loss: categorical_crossentropy\n",
    "- Batch size: 32\n",
    "- Epochs: 30\n",
    "- Callbacks: EarlyStopping (patience=5)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Build the neural network model\n",
    "def build_model(input_dim=9):\n",
    "    \"\"\"\n",
    "    Build the Neural Execution Risk Predictor model\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer (9 features)\n",
    "    - Dense(64, ReLU)\n",
    "    - Dropout(0.2)\n",
    "    - Dense(32, ReLU)\n",
    "    - Output Dense(3, Softmax)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu', name='hidden_layer_1'),\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        layers.Dense(32, activation='relu', name='hidden_layer_2'),\n",
    "        layers.Dense(3, activation='softmax', name='output_layer')\n",
    "    ], name='NeuralExecutionRiskPredictor')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = build_model(input_dim=X_train_scaled.shape[1])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Model Training\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Save the trained model\n",
    "model_path = os.path.join(MODEL_DIR, 'risk_model.h5')\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Also save in SavedModel format for production\n",
    "saved_model_dir = os.path.join(MODEL_DIR, 'risk_model_saved')\n",
    "model.save(saved_model_dir, save_format='tf')\n",
    "print(f\"SavedModel format saved to: {saved_model_dir}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5. Model Evaluation\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate metrics per class\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Class':<15} {'Precision':<12} {'Recall':<12}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'LOW_RISK (0)':<15} {precision[0]:<12.4f} {recall[0]:<12.4f}\")\n",
    "print(f\"{'MEDIUM_RISK (1)':<15} {precision[1]:<12.4f} {recall[1]:<12.4f}\")\n",
    "print(f\"{'HIGH_RISK (2)':<15} {precision[2]:<12.4f} {recall[2]:<12.4f}\")\n",
    "\n",
    "# Overall metrics\n",
    "print(f\"\\n{'Overall':<15} {precision.mean():<12.4f} {recall.mean():<12.4f}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "target_names = ['LOW_RISK', 'MEDIUM_RISK', 'HIGH_RISK']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "print(cm)\n",
    "print(\"\\nRows = Actual, Columns = Predicted\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Error Analysis\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Error Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# False Positives: Predicted higher risk than actual\n",
    "false_positives = {\n",
    "    'LOWâ†’MEDIUM': cm[0, 1],\n",
    "    'LOWâ†’HIGH': cm[0, 2],\n",
    "    'MEDIUMâ†’HIGH': cm[1, 2]\n",
    "}\n",
    "\n",
    "# False Negatives: Predicted lower risk than actual\n",
    "false_negatives = {\n",
    "    'MEDIUMâ†’LOW': cm[1, 0],\n",
    "    'HIGHâ†’LOW': cm[2, 0],\n",
    "    'HIGHâ†’MEDIUM': cm[2, 1]\n",
    "}\n",
    "\n",
    "print(\"\\nFalse Positives (over-estimation of risk):\")\n",
    "for transition, count in false_positives.items():\n",
    "    print(f\"  {transition}: {count}\")\n",
    "\n",
    "print(\"\\nFalse Negatives (under-estimation of risk):\")\n",
    "for transition, count in false_negatives.items():\n",
    "    print(f\"  {transition}: {count}\")\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "- False Positives: Model predicts HIGHER risk than actual\n",
    "  â†’ Impact: Agent might be unnecessarily restricted\n",
    "  â†’ Safety: SAFER (conservative approach)\n",
    "  \n",
    "- False Negatives: Model predicts LOWER risk than actual\n",
    "  â†’ Impact: Agent runs with insufficient guards\n",
    "  â†’ Safety: DANGEROUS (could lead to failures)\n",
    "  \n",
    "For a production Runtime Guard:\n",
    "- False negatives (HIGHâ†’LOW, HIGHâ†’MEDIUM) are MORE CRITICAL\n",
    "- They represent missed high-risk scenarios\n",
    "- Current model should prioritize minimizing these errors\n",
    "\"\"\")\n",
    "\n",
    "# Calculate critical error rate\n",
    "critical_errors = cm[2, 0] + cm[2, 1]  # HIGH predicted as LOW or MEDIUM\n",
    "total_high_risk = cm[2, :].sum()\n",
    "if total_high_risk > 0:\n",
    "    critical_error_rate = critical_errors / total_high_risk\n",
    "    print(f\"\\nCRITICAL ERROR RATE (missed HIGH_RISK): {critical_error_rate:.2%}\")\n",
    "    print(f\"  â†’ {critical_errors} out of {total_high_risk} high-risk cases were underestimated\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6. Visualization\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Plot 1: Training & Validation Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(REPORTS_DIR, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Training curves saved!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Plot: Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['LOW', 'MEDIUM', 'HIGH'],\n",
    "            yticklabels=['LOW', 'MEDIUM', 'HIGH'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Risk Level', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual Risk Level', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix - Neural Execution Risk Predictor', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(REPORTS_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Confusion matrix heatmap saved!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Using **Permutation Importance** to understand which features most influence predictions.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Build a wrapper for permutation importance (needs sklearn-compatible predict)\n",
    "class KerasClassifierWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probs = self.model.predict(X, verbose=0)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Accuracy score\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "# Wrap the model\n",
    "wrapped_model = KerasClassifierWrapper(model)\n",
    "\n",
    "# Calculate permutation importance\n",
    "print(\"Calculating permutation importance...\")\n",
    "print(\"(This may take a minute...)\\n\")\n",
    "\n",
    "perm_importance = permutation_importance(\n",
    "    wrapped_model, \n",
    "    X_test_scaled, \n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Get importance scores\n",
    "importance_mean = perm_importance.importances_mean\n",
    "importance_std = perm_importance.importances_std\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance_mean,\n",
    "    'Std': importance_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE (Permutation)\")\n",
    "print(\"=\"*60)\n",
    "print(importance_df.to_string(index=False))\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Plot Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], \n",
    "         xerr=importance_df['Std'], capsize=5, color='steelblue', alpha=0.8)\n",
    "plt.xlabel('Importance (Drop in Accuracy)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Feature Importance - Neural Execution Risk Predictor', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(REPORTS_DIR, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Feature importance plot saved!\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Feature Importance Interpretation\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_features = importance_df.head(3)['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nTop 3 Most Important Features:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"SYSTEMS ENGINEERING PERSPECTIVE:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "The most important features for predicting HIGH_RISK likely include:\n",
    "\n",
    "1. **est_tokens**: Token budget directly correlates with computational cost\n",
    "   - High token usage â†’ resource exhaustion risk\n",
    "   - Critical for LLM-based agents\n",
    "\n",
    "2. **num_steps**: Execution complexity indicator\n",
    "   - More steps â†’ higher chance of failure propagation\n",
    "   - Long execution chains are inherently riskier\n",
    "\n",
    "3. **sequential_tool_calls**: Indicator of retry loops or stuck execution\n",
    "   - Repeated tool calls suggest potential infinite loops\n",
    "   - Sign of planning failures or environmental issues\n",
    "\n",
    "4. **has_high_risk_tool**: Explicit risk flagging\n",
    "   - Certain tools (file operations, network calls) are inherently risky\n",
    "   - Binary indicator with high signal value\n",
    "\n",
    "These features make sense because they capture:\n",
    "- Resource consumption (tokens, time)\n",
    "- Execution complexity (steps, depth)\n",
    "- Failure patterns (retries, loops)\n",
    "- Explicit risk markers (dangerous tools)\n",
    "\n",
    "This aligns with production agent monitoring requirements.\n",
    "\"\"\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 8. Model Export & Metadata\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Save feature names for API\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'model_name': 'Neural Execution Risk Predictor',\n",
    "    'version': '1.0',\n",
    "    'input_features': feature_names,\n",
    "    'output_classes': {\n",
    "        0: 'LOW_RISK',\n",
    "        1: 'MEDIUM_RISK',\n",
    "        2: 'HIGH_RISK'\n",
    "    },\n",
    "    'architecture': {\n",
    "        'layers': ['Dense(64, ReLU)', 'Dropout(0.2)', 'Dense(32, ReLU)', 'Dense(3, Softmax)'],\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.001,\n",
    "        'loss': 'categorical_crossentropy'\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_loss': float(test_loss),\n",
    "        'precision_per_class': precision.tolist(),\n",
    "        'recall_per_class': recall.tolist()\n",
    "    },\n",
    "    'feature_importance': importance_df.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(MODEL_DIR, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model metadata saved to: {metadata_path}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 9. Test Inference (Sample Predictions)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"code\">\n",
    "# Test with sample inputs\n",
    "test_samples = [\n",
    "    {\n",
    "        'name': 'Simple Safe Execution',\n",
    "        'num_steps': 3,\n",
    "        'num_tools': 2,\n",
    "        'tool_diversity': 2,\n",
    "        'has_high_risk_tool': 0,\n",
    "        'est_tokens': 1500,\n",
    "        'max_retries': 1,\n",
    "        'sequential_tool_calls': 0,\n",
    "        'plan_depth': 1,\n",
    "        'time_limit_sec': 60\n",
    "    },\n",
    "    {\n",
    "        'name': 'Moderate Complexity',\n",
    "        'num_steps': 8,\n",
    "        'num_tools': 4,\n",
    "        'tool_diversity': 4,\n",
    "        'has_high_risk_tool': 1,\n",
    "        'est_tokens': 6000,\n",
    "        'max_retries': 3,\n",
    "        'sequential_tool_calls': 2,\n",
    "        'plan_depth': 2,\n",
    "        'time_limit_sec': 180\n",
    "    },\n",
    "    {\n",
    "        'name': 'High-Risk Execution',\n",
    "        'num_steps': 18,\n",
    "        'num_tools': 8,\n",
    "        'tool_diversity': 7,\n",
    "        'has_high_risk_tool': 1,\n",
    "        'est_tokens': 15000,\n",
    "        'max_retries': 6,\n",
    "        'sequential_tool_calls': 12,\n",
    "        'plan_depth': 4,\n",
    "        'time_limit_sec': 500\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for sample in test_samples:\n",
    "    name = sample.pop('name')\n",
    "    \n",
    "    # Prepare input\n",
    "    input_array = np.array([[\n",
    "        sample['num_steps'],\n",
    "        sample['num_tools'],\n",
    "        sample['tool_diversity'],\n",
    "        sample['has_high_risk_tool'],\n",
    "        sample['est_tokens'],\n",
    "        sample['max_retries'],\n",
    "        sample['sequential_tool_calls'],\n",
    "        sample['plan_depth'],\n",
    "        sample['time_limit_sec']\n",
    "    ]])\n",
    "    \n",
    "    # Scale input\n",
    "    input_scaled = scaler.transform(input_array)\n",
    "    \n",
    "    # Predict\n",
    "    prediction_probs = model.predict(input_scaled, verbose=0)[0]\n",
    "    predicted_class = np.argmax(prediction_probs)\n",
    "    risk_score = float(prediction_probs[predicted_class])\n",
    "    \n",
    "    risk_labels = ['LOW_RISK', 'MEDIUM_RISK', 'HIGH_RISK']\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Input: {sample}\")\n",
    "    print(f\"  Prediction: {risk_labels[predicted_class]}\")\n",
    "    print(f\"  Confidence: {risk_score:.2%}\")\n",
    "    print(f\"  Risk Distribution: LOW={prediction_probs[0]:.3f}, MED={prediction_probs[1]:.3f}, HIGH={prediction_probs[2]:.3f}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 10. Summary & Next Steps\n",
    "\n",
    "### What We Built:\n",
    "âœ… Hybrid dataset from BPI Challenge 2012 + Synthetic plans  \n",
    "âœ… 9-feature tabular input (NO NLP)  \n",
    "âœ… Deep Neural Network (64â†’32â†’3 architecture)  \n",
    "âœ… Training with early stopping  \n",
    "âœ… Comprehensive evaluation & error analysis  \n",
    "âœ… Feature importance analysis  \n",
    "âœ… Production-ready model artifacts  \n",
    "\n",
    "### Model Performance:\n",
    "- **Test Accuracy**: {test_accuracy:.2%}\n",
    "- **Key Insight**: Model balances safety (avoiding false negatives) with efficiency\n",
    "\n",
    "### Integration with Agent Runtime Guard:\n",
    "\n",
    "```python\n",
    "# Pseudo-code for Runtime Guard Integration\n",
    "class AgentRuntimeGuard:\n",
    "    def __init__(self, risk_model):\n",
    "        self.risk_model = risk_model\n",
    "        \n",
    "    def evaluate_plan(self, execution_plan):\n",
    "        # Extract features from plan\n",
    "        features = extract_features(execution_plan)\n",
    "        \n",
    "        # Predict risk\n",
    "        risk_level, risk_score = self.risk_model.predict(features)\n",
    "        \n",
    "        # Apply guardrails\n",
    "        if risk_level == 'HIGH_RISK':\n",
    "            return self.apply_tight_limits(execution_plan)\n",
    "        elif risk_level == 'MEDIUM_RISK':\n",
    "            return self.apply_moderate_limits(execution_plan)\n",
    "        else:\n",
    "            return self.allow_normal_execution(execution_plan)\n",
    "```\n",
    "\n",
    "### Files Generated:\n",
    "- `model/risk_model.h5` - Trained model\n",
    "- `model/scaler.joblib` - Feature scaler\n",
    "- `model/model_metadata.json` - Model metadata\n",
    "- `reports/*.png` - Visualizations\n",
    "\n",
    "### Next Steps:\n",
    "1. Build FastAPI service (see `api/main.py`)\n",
    "2. Dockerize the application\n",
    "3. Deploy to production\n",
    "4. Monitor model performance in real-time\n",
    "5. Retrain with production data\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "**Project Complete!** ðŸŽ‰\n",
    "\n",
    "You now have a production-ready Deep Learning system for predicting agent execution risk.\n",
    "</VSCode.Cell>\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
