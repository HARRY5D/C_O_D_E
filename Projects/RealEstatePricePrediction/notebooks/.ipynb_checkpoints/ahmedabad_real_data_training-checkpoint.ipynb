{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af31bfe",
   "metadata": {},
   "source": [
    "# Real Estate Price Prediction - Ahmedabad Real Data Training üè†üáÆüá≥\n",
    "\n",
    "## Overview\n",
    "This notebook trains machine learning models on **real Ahmedabad property data** from the provided CSV file. The dataset contains actual property listings with prices in Indian Rupees.\n",
    "\n",
    "### Dataset Information\n",
    "- **Source**: Real Ahmedabad property listings\n",
    "- **Size**: 6,855+ properties\n",
    "- **Currency**: Indian Rupees (‚Çπ)\n",
    "- **Areas**: Vastrapur, Bopal, Shela, Satellite, Maninagar, and more\n",
    "- **Features**: BHK, area, location, price, furnishing, status\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Import our custom data processor\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from real_data_processor import RealDataProcessor\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üè† Ready to process real Ahmedabad property data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cabf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Process Real Ahmedabad Data\n",
    "print(\"üè† Loading Real Ahmedabad Property Data...\")\n",
    "\n",
    "# Load the real dataset\n",
    "df_raw = pd.read_csv('../data/ahmedabad.csv')\n",
    "print(f\"‚úÖ Loaded {len(df_raw)} real properties from CSV\")\n",
    "\n",
    "# Display raw data info\n",
    "print(\"\\nüìä Raw Dataset Info:\")\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "\n",
    "# Show sample raw data\n",
    "print(\"\\nüîç Sample Raw Data:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(df_raw.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3753f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Real Data\n",
    "print(\"üîß Processing real Ahmedabad property data...\")\n",
    "\n",
    "# Initialize processor\n",
    "processor = RealDataProcessor()\n",
    "\n",
    "# Process the raw data\n",
    "df = processor.process_real_ahmedabad_data(df_raw)\n",
    "\n",
    "print(f\"\\nüìä Processed dataset shape: {df.shape}\")\n",
    "print(f\"üí∞ Price range: ‚Çπ{df['price'].min():,.0f} - ‚Çπ{df['price'].max():,.0f}\")\n",
    "print(f\"üèòÔ∏è Areas covered: {', '.join(df['neighborhood'].unique()[:8])}...\")\n",
    "\n",
    "# Display processed data info\n",
    "print(\"\\nüîç Processed Data Sample:\")\n",
    "print(df[['price', 'bedrooms', 'square_feet', 'neighborhood', 'property_type']].head(10))\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nüìà Basic Statistics:\")\n",
    "print(df[['price', 'bedrooms', 'bathrooms', 'square_feet', 'property_age']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ce903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "print(\"üìä Performing Exploratory Data Analysis...\")\n",
    "\n",
    "# Create subplots for EDA\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Ahmedabad Real Estate - Exploratory Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Price distribution\n",
    "axes[0, 0].hist(df['price']/100000, bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Price Distribution (in Lac ‚Çπ)')\n",
    "axes[0, 0].set_xlabel('Price (Lac ‚Çπ)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Bedrooms distribution\n",
    "bedroom_counts = df['bedrooms'].value_counts().sort_index()\n",
    "axes[0, 1].bar(bedroom_counts.index, bedroom_counts.values, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Bedrooms Distribution')\n",
    "axes[0, 1].set_xlabel('Number of Bedrooms')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Square feet distribution\n",
    "axes[0, 2].hist(df['square_feet'], bins=50, alpha=0.7, color='salmon')\n",
    "axes[0, 2].set_title('Square Feet Distribution')\n",
    "axes[0, 2].set_xlabel('Square Feet')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# Price by neighborhood (top 10)\n",
    "top_neighborhoods = df['neighborhood'].value_counts().head(10).index\n",
    "neighborhood_prices = df[df['neighborhood'].isin(top_neighborhoods)].groupby('neighborhood')['price'].mean().sort_values(ascending=False)\n",
    "axes[1, 0].bar(range(len(neighborhood_prices)), neighborhood_prices.values/100000, alpha=0.7, color='gold')\n",
    "axes[1, 0].set_title('Average Price by Neighborhood (Top 10)')\n",
    "axes[1, 0].set_xlabel('Neighborhood')\n",
    "axes[1, 0].set_ylabel('Average Price (Lac ‚Çπ)')\n",
    "axes[1, 0].set_xticks(range(len(neighborhood_prices)))\n",
    "axes[1, 0].set_xticklabels(neighborhood_prices.index, rotation=45, ha='right')\n",
    "\n",
    "# Property type distribution\n",
    "prop_type_counts = df['property_type'].value_counts()\n",
    "axes[1, 1].pie(prop_type_counts.values, labels=prop_type_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Property Type Distribution')\n",
    "\n",
    "# Price vs Square Feet scatter\n",
    "axes[1, 2].scatter(df['square_feet'], df['price']/100000, alpha=0.5, color='purple')\n",
    "axes[1, 2].set_title('Price vs Square Feet')\n",
    "axes[1, 2].set_xlabel('Square Feet')\n",
    "axes[1, 2].set_ylabel('Price (Lac ‚Çπ)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(f\"\\nüîç Key Insights:\")\n",
    "print(f\"   üí∞ Average price: ‚Çπ{df['price'].mean():,.0f} ({df['price'].mean()/100000:.1f} Lac)\")\n",
    "print(f\"   üè† Most common: {df['bedrooms'].mode()[0]:.0f} BHK properties\")\n",
    "print(f\"   üìê Average size: {df['square_feet'].mean():.0f} sq ft\")\n",
    "print(f\"   üèòÔ∏è Most expensive area: {neighborhood_prices.index[0]}\")\n",
    "print(f\"   üèóÔ∏è Most common type: {df['property_type'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Machine Learning\n",
    "print(\"ü§ñ Preparing data for machine learning...\")\n",
    "\n",
    "# Prepare features\n",
    "X, y = processor.prepare_features_for_ml(df)\n",
    "print(f\"‚úÖ Features prepared: {X.shape[1]} features, {len(X)} samples\")\n",
    "print(f\"üìä Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = processor.create_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"\\nüìä Data split:\")\n",
    "print(f\"   üéØ Training set: {len(X_train)} samples\")\n",
    "print(f\"   üß™ Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Scale features for some algorithms\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(f\"   üí∞ Training price range: ‚Çπ{y_train.min():,.0f} - ‚Çπ{y_train.max():,.0f}\")\n",
    "print(f\"   üéØ Test price range: ‚Çπ{y_test.min():,.0f} - ‚Çπ{y_test.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multiple ML Models\n",
    "print(\"ü§ñ Training Multiple ML Models on Real Ahmedabad Data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'Support Vector Regression': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate all models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nü§ñ Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for SVR and linear models\n",
    "    if name in ['Support Vector Regression', 'Ridge Regression', 'Lasso Regression']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    test_mape = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mape': test_mape,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"   üìä Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"   üìà Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"   üí∞ Test MAE: ‚Çπ{test_mae:,.0f} ({test_mae/100000:.1f} Lac)\")\n",
    "    print(f\"   üìä Test MAPE: {test_mape:.1f}%\")\n",
    "    print(f\"   üéØ CV Score: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Comparison and Analysis\n",
    "print(\"üìä Model Performance Analysis...\")\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "performance_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test_R2': [results[name]['test_r2'] for name in results.keys()],\n",
    "    'Test_MAE': [results[name]['test_mae'] for name in results.keys()],\n",
    "    'Test_RMSE': [results[name]['test_rmse'] for name in results.keys()],\n",
    "    'Test_MAPE': [results[name]['test_mape'] for name in results.keys()],\n",
    "    'CV_Mean': [results[name]['cv_mean'] for name in results.keys()],\n",
    "    'CV_Std': [results[name]['cv_std'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "performance_df = performance_df.sort_values('Test_R2', ascending=False)\n",
    "print(\"\\nüèÜ Model Performance Ranking:\")\n",
    "print(performance_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = performance_df.iloc[0]['Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nü•á Best Model: {best_model_name}\")\n",
    "print(f\"   üìà Test R¬≤: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"   üí∞ Test MAE: ‚Çπ{results[best_model_name]['test_mae']:,.0f}\")\n",
    "print(f\"   üìä Test MAPE: {results[best_model_name]['test_mape']:.1f}%\")\n",
    "\n",
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# R¬≤ Score comparison\n",
    "axes[0].bar(performance_df['Model'], performance_df['Test_R2'], alpha=0.7, color='skyblue')\n",
    "axes[0].set_title('Model R¬≤ Score Comparison')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(performance_df['Model'], performance_df['Test_MAE']/100000, alpha=0.7, color='lightgreen')\n",
    "axes[1].set_title('Model MAE Comparison')\n",
    "axes[1].set_ylabel('MAE (Lac ‚Çπ)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAPE comparison\n",
    "axes[2].bar(performance_df['Model'], performance_df['Test_MAPE'], alpha=0.7, color='salmon')\n",
    "axes[2].set_title('Model MAPE Comparison')\n",
    "axes[2].set_ylabel('MAPE (%)')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ace1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "print(\"üîç Feature Importance Analysis...\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top 15 Most Important Features ({best_model_name}):\")\n",
    "    print(feature_importance.head(15))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(top_features['feature'], top_features['importance'], alpha=0.7, color='gold')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance insights\n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    print(f\"   ü•á Most important: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.3f})\")\n",
    "    print(f\"   ü•à Second most: {feature_importance.iloc[1]['feature']} ({feature_importance.iloc[1]['importance']:.3f})\")\n",
    "    print(f\"   ü•â Third most: {feature_importance.iloc[2]['feature']} ({feature_importance.iloc[2]['importance']:.3f})\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Feature importance not available for {best_model_name}\")\n",
    "    \n",
    "    # For linear models, show coefficients\n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        coef_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'coefficient': best_model.coef_\n",
    "        })\n",
    "        coef_importance['abs_coefficient'] = np.abs(coef_importance['coefficient'])\n",
    "        coef_importance = coef_importance.sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä Top 10 Features by Coefficient Magnitude ({best_model_name}):\")\n",
    "        print(coef_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df993b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Predictions Visualization\n",
    "print(\"üìä Visualizing Model Predictions...\")\n",
    "\n",
    "# Get predictions from best model\n",
    "if best_model_name in ['Support Vector Regression', 'Ridge Regression', 'Lasso Regression']:\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(f'Model Prediction Analysis - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Actual vs Predicted scatter plot\n",
    "axes[0, 0].scatter(y_test/100000, y_pred_best/100000, alpha=0.6, color='blue')\n",
    "axes[0, 0].plot([y_test.min()/100000, y_test.max()/100000], \n",
    "                [y_test.min()/100000, y_test.max()/100000], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Price (Lac ‚Çπ)')\n",
    "axes[0, 0].set_ylabel('Predicted Price (Lac ‚Çπ)')\n",
    "axes[0, 0].set_title('Actual vs Predicted Prices')\n",
    "axes[0, 0].text(0.05, 0.95, f'R¬≤ = {results[best_model_name][\"test_r2\"]:.4f}', \n",
    "                transform=axes[0, 0].transAxes, fontsize=12, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred_best\n",
    "axes[0, 1].scatter(y_pred_best/100000, residuals/100000, alpha=0.6, color='green')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Price (Lac ‚Çπ)')\n",
    "axes[0, 1].set_ylabel('Residuals (Lac ‚Çπ)')\n",
    "axes[0, 1].set_title('Residuals Plot')\n",
    "\n",
    "# Prediction error distribution\n",
    "axes[1, 0].hist(residuals/100000, bins=30, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_xlabel('Prediction Error (Lac ‚Çπ)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Prediction Error Distribution')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "# Percentage error distribution\n",
    "percentage_errors = ((y_test - y_pred_best) / y_test) * 100\n",
    "axes[1, 1].hist(percentage_errors, bins=30, alpha=0.7, color='purple')\n",
    "axes[1, 1].set_xlabel('Percentage Error (%)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Percentage Error Distribution')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print prediction statistics\n",
    "print(f\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"   üìà Mean Absolute Error: ‚Çπ{results[best_model_name]['test_mae']:,.0f}\")\n",
    "print(f\"   üìä Root Mean Square Error: ‚Çπ{results[best_model_name]['test_rmse']:,.0f}\")\n",
    "print(f\"   üéØ Mean Absolute Percentage Error: {results[best_model_name]['test_mape']:.1f}%\")\n",
    "print(f\"   üìà R¬≤ Score: {results[best_model_name]['test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Predictions with Real Property Details\n",
    "print(\"üè† Testing Model with Sample Predictions...\")\n",
    "\n",
    "# Select random samples for testing\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(X_test.index, size=10, replace=False)\n",
    "sample_X = X_test.loc[sample_indices]\n",
    "sample_y_actual = y_test.loc[sample_indices]\n",
    "\n",
    "if best_model_name in ['Support Vector Regression', 'Ridge Regression', 'Lasso Regression']:\n",
    "    sample_y_pred = best_model.predict(scaler.transform(sample_X))\n",
    "else:\n",
    "    sample_y_pred = best_model.predict(sample_X)\n",
    "\n",
    "print(\"\\nüè† Sample Predictions vs Actual (Real Ahmedabad Properties):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    actual = sample_y_actual.loc[idx]\n",
    "    predicted = sample_y_pred[i]\n",
    "    error = abs(actual - predicted)\n",
    "    error_pct = (error / actual) * 100\n",
    "    \n",
    "    # Get property details from processed dataframe\n",
    "    property_details = df.loc[idx]\n",
    "    \n",
    "    print(f\"\\nüè† Property {i+1}:\")\n",
    "    print(f\"   üìç Location: {property_details['neighborhood']}\")\n",
    "    print(f\"   üè† Type: {property_details['property_type']}\")\n",
    "    print(f\"   üõèÔ∏è Bedrooms: {property_details['bedrooms']:.0f} BHK\")\n",
    "    print(f\"   üìê Size: {property_details['square_feet']:.0f} sq ft\")\n",
    "    print(f\"   üèóÔ∏è Age: {property_details['property_age']:.0f} years\")\n",
    "    print(f\"   üí∞ Actual Price: ‚Çπ{actual:,.0f} ({actual/100000:.1f} Lac)\")\n",
    "    print(f\"   ü§ñ Predicted Price: ‚Çπ{predicted:,.0f} ({predicted/100000:.1f} Lac)\")\n",
    "    print(f\"   üìä Error: ‚Çπ{error:,.0f} ({error_pct:.1f}%)\")\n",
    "    \n",
    "    # Add accuracy indicator\n",
    "    if error_pct < 10:\n",
    "        print(f\"   ‚úÖ Excellent prediction!\")\n",
    "    elif error_pct < 20:\n",
    "        print(f\"   üëç Good prediction\")\n",
    "    elif error_pct < 30:\n",
    "        print(f\"   ‚ö†Ô∏è Fair prediction\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Poor prediction\")\n",
    "\n",
    "# Calculate sample statistics\n",
    "sample_mae = mean_absolute_error(sample_y_actual, sample_y_pred)\n",
    "sample_mape = np.mean(np.abs((sample_y_actual - sample_y_pred) / sample_y_actual)) * 100\n",
    "sample_r2 = r2_score(sample_y_actual, sample_y_pred)\n",
    "\n",
    "print(f\"\\nüìä Sample Performance:\")\n",
    "print(f\"   üí∞ Sample MAE: ‚Çπ{sample_mae:,.0f} ({sample_mae/100000:.1f} Lac)\")\n",
    "print(f\"   üìä Sample MAPE: {sample_mape:.1f}%\")\n",
    "print(f\"   üìà Sample R¬≤: {sample_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Best Model and Results\n",
    "print(\"üíæ Saving Model and Results...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_filename = f'../models/best_ahmedabad_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"‚úÖ Best model saved: {model_filename}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = '../models/feature_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"‚úÖ Feature scaler saved: {scaler_filename}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_filename = '../models/feature_names.pkl'\n",
    "joblib.dump(list(X.columns), feature_names_filename)\n",
    "print(f\"‚úÖ Feature names saved: {feature_names_filename}\")\n",
    "\n",
    "# Save model performance results\n",
    "results_filename = '../models/model_performance_results.csv'\n",
    "performance_df.to_csv(results_filename, index=False)\n",
    "print(f\"‚úÖ Performance results saved: {results_filename}\")\n",
    "\n",
    "# Create a model summary\n",
    "model_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'test_r2': results[best_model_name]['test_r2'],\n",
    "    'test_mae': results[best_model_name]['test_mae'],\n",
    "    'test_mape': results[best_model_name]['test_mape'],\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features_count': X.shape[1],\n",
    "    'dataset_size': len(df),\n",
    "    'price_range_min': float(df['price'].min()),\n",
    "    'price_range_max': float(df['price'].max())\n",
    "}\n",
    "\n",
    "import json\n",
    "summary_filename = '../models/model_summary.json'\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "print(f\"‚úÖ Model summary saved: {summary_filename}\")\n",
    "\n",
    "print(f\"\\nüéâ Model Training Complete!\")\n",
    "print(f\"\\nüìä Final Results Summary:\")\n",
    "print(f\"   ü•á Best Model: {best_model_name}\")\n",
    "print(f\"   üìà R¬≤ Score: {results[best_model_name]['test_r2']:.4f}\")\n",
    "print(f\"   üí∞ Mean Absolute Error: ‚Çπ{results[best_model_name]['test_mae']:,.0f}\")\n",
    "print(f\"   üìä Mean Absolute Percentage Error: {results[best_model_name]['test_mape']:.1f}%\")\n",
    "print(f\"   üè† Trained on {len(df)} real Ahmedabad properties\")\n",
    "print(f\"   üîç Using {X.shape[1]} features\")\n",
    "print(f\"\\nüíæ All model artifacts saved in '../models/' directory\")\n",
    "print(f\"üöÄ Ready for deployment and predictions!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
